[{"content":"\nNetworking in GCP, or any cloud environment, often requires a load balancer(LB). If you try to use Terraform to deploy a load balancer in GCP you\u0026rsquo;ll notice that there is not a single Terraform resource to create a GCP LB. GCP provides example Terraform scripts to create LBs in their documentation here.\nHowever, if you have an understanding of how load balancers work internally, and how to properly structure your Terraform code, creating your own LB modules in GCP using Terraform is possible. This can allow you to configure your LBs to do exactly what you need.\nWhat is a Load Balancer? Load balancers are networking resources that take in network traffic through their frontend and distribute that traffic to their backend(s). Load balancers have a few basic parts you should know:\nFrontend Where clients reach the load balancer, the \u0026ldquo;face\u0026rdquo; of the load balancer.\nBackend The server(s) where the load balancer directs traffic\nForwarding Rules The logic the load balancer uses to distribute traffic. Also referred to as a load balancer\u0026rsquo;s \u0026ldquo;algorithm\u0026rdquo;.\nHealth Checks Checks the status of the backend(s) and whether they can recieve traffic. Load balancers will not send traffic to an unhealthy backend.\nBelow is a simple diagram that illustrates the flow of traffic through a load balancer: Traffic enters the load balancer through its frontend IP, the forwarding rule(s) determines how to route traffic to the backends, the health check confirms that the backend is healthy-and if so then the traffic is routed there.\nRules Deciding how to route traffic to a load balancer\u0026rsquo;s backends depends on the use case. For example, if a network admin has 2 servers and server 1 can handle more requests than server 2, then the load balancer can be configured to use a weighted rule which will send more traffic to server 1.\nHowever if a network admin has 2 servers with the same processing power they can confgure the load balancer to use a least connection rule which will send traffic to the server that is the most available at the time.\nGCP Load Balancers Load balancers can exist in the cloud as well as on-premises networks. This post will primarily focus on internal/external TCP/HTTPS cloud-based load balancers in GCP, although similar load balancers exist in AWS and Azure cloud platforms. Load balancers in GCP work just like load balancers in any other network, with the exception that some or all of the traffic of GCP load balancers is between GCP cloud resources.\nWhen creating a load balancer in GCP, it is important to understand the different types of load balancers supported.\nInternal and External Load Balancers GCP supports 2 options for the IP address of their load balancers, internal and external.\nInternal LBs only distribute traffic to resources within GCP. External LBs route traffic coming from the internet into GCP. It is important to note that external LBs often require additional security measures due to the LBs being public-facing.\nTCP/UDP and HTTP(S) Load Balancers GCP supports 4 types of traffic on their load balancers: TCP, UDP, HTTP(S), and SSL. This post will primarily focus on TCP/UDP and HTTP(S) load balancing.\nCreating a Load Balancer using Terraform Load balancers can be created in GCP using Terraform, however there is not a single Terraform resource that creates a load balancer. Instead, multiple Terraform resources together create a GCP load balancer. These resources include:\n\u0026ldquo;google_compute_global_address\u0026rdquo; = The public IP address that is the frontend of the LB. (External LBs only)\n\u0026ldquo;google_compute_target_http_proxy\u0026rdquo; = Routes requests to the url map (\u0026ldquo;google_compute_global_forwarding_rule\u0026rdquo; for external LBs)\n\u0026ldquo;google_compute_url_map\u0026rdquo; = Defines the rules to route traffic to the backend\n\u0026ldquo;google_compute_forwarding_rule\u0026rdquo; = Routes traffic to the backends\n\u0026ldquo;google_compute_backend_service\u0026rdquo; = Creates the LB\u0026rsquo;s backend(s)\n\u0026ldquo;google_compute_health_check\u0026rdquo; = The LB\u0026rsquo;s health check (optiona)\nNote, that the global address and http proxy resources are different between internal and external LBs. External LBs use a global address resource to make a public IP address that clients from the internet can use to access the external LB. Internal LBs do not need a global address. In addiiton, internal LBs use the \u0026ldquo;google_compute_target_http_proxy\u0026rdquo; proxy resource, external LBs use the \u0026ldquo;google_compute_global_forwarding_rule\u0026rdquo; resource.\nBelow is a diagram illustrating the flow of traffic through the Terraform resources that make an internal load balancer: And this is the flow of traffic through the Terraform resources that make an external load balancer: URL Maps in a GCP LB Created with Terraform It is simple enough to copy/paste the required TF resources to create a GCP LB, but configuring the LB to route traffic in the way that you want isn\u0026rsquo;t as straightforward. A URL map is used to send requests to the correct backend based on logic that you define.\nHost and Path Rules Creating rules based on the host and path of a request allows you to route traffic coming from specific points to specific backends.\nHost rules match the host name of a request, like \u0026ldquo;example.com\u0026rdquo;. Each host rule has a path matcher rule that specifies the what path from that host should go to what backend. The most simple path mather rule is \u0026ldquo;/*\u0026rdquo; which matches all paths.\nThe \u0026ldquo;google_compute_url_map\u0026rdquo; resource accepts host and path matcher variables, for example:\nresource \u0026#34;google_compute_url_map\u0026#34; \u0026#34;urlmap\u0026#34; { name = \u0026#34;my-urlmap\u0026#34; description = \u0026#34;Example url map\u0026#34; default_service = google_compute_backend_bucket.static.id host_rule { hosts = [\u0026#34;example.com\u0026#34;] path_matcher = \u0026#34;example\u0026#34; } path_matcher { name = \u0026#34;example\u0026#34; default_service = google_compute_backend_bucket.home.id path_rule { paths = [\u0026#34;/home\u0026#34;] service = google_compute_backend_bucket.home.id } path_rule { paths = [\u0026#34;/login\u0026#34;] service = google_compute_backend_service.login.id } path_rule { paths = [\u0026#34;/static\u0026#34;] service = google_compute_backend_bucket.static.id } } } In the example you can see that the host rule will match requests for the host \u0026ldquo;example.com\u0026rdquo;. That host rule references the path_matcher \u0026ldquo;example\u0026rdquo;, which contains 3 path rules. Those path rules send traffic to a certain backend depending on what path the request is for. So a request for \u0026ldquo;example.com/home\u0026rdquo; will go to the backend google_compute_backend_bucket.home.\nIn addition to host/path rules, the url map resource accepts many other arguments you may want to use for your load balancer. The Terraform registry includes a complete list of arguments.\nExample Below is a basic example of Terraform code that creates an external HTTPS load balancer in GCP that uses a NEG (network endpoint group) as a backend.\n# Create network endpoint resource \u0026#34;google_compute_global_network_endpoint\u0026#34; \u0026#34;default\u0026#34; { global_network_endpoint_group = google_compute_global_network_endpoint_group.default.name fqdn = \u0026#34;www.example.com\u0026#34; port = 90 } # Add network endpoint to network endpoint group resource \u0026#34;google_compute_global_network_endpoint_group\u0026#34; \u0026#34;default\u0026#34; { name = \u0026#34;example-neg\u0026#34; default_port = \u0026#34;90\u0026#34; network_endpoint_type = \u0026#34;INTERNET_FQDN_PORT\u0026#34; } # Since this is an external LB it needs a SSL certificate resource \u0026#34;google_compute_managed_ssl_certificate\u0026#34; \u0026#34;default\u0026#34; { name = \u0026#34;example-cert\u0026#34; managed { domains = [\u0026#34;example\u0026#34;] } } # Forwarding rule resource \u0026#34;google_compute_global_forwarding_rule\u0026#34; \u0026#34;example\u0026#34; { name = \u0026#34;example-forwarding-rule\u0026#34; provider = google-beta port_range = \u0026#34;443\u0026#34; target = google_compute_target_https_proxy.default.self_link ip_address = google_compute_global_address.default.address } # Public IP address resource \u0026#34;google_compute_global_address\u0026#34; \u0026#34;default\u0026#34; { provider = google-beta name = \u0026#34;example-address\u0026#34; ip_version = \u0026#34;IPV4\u0026#34; } # HTTPS proxy resource \u0026#34;google_compute_target_https_proxy\u0026#34; \u0026#34;default\u0026#34; { name = \u0026#34;example-proxy\u0026#34; url_map = google_compute_url_map.default.id ssl_certificates = [google_compute_managed_ssl_certificate.default.id] } # URL Map resource \u0026#34;google_compute_url_map\u0026#34; \u0026#34;default\u0026#34; { name = \u0026#34;example-urlmap\u0026#34; description = \u0026#34;Example url map\u0026#34; default_service = google_compute_backend_service.default.id } # Make NEG the backend resource \u0026#34;google_compute_backend_service\u0026#34; \u0026#34;default\u0026#34; { name = \u0026#34;example-backend-service\u0026#34; port_name = \u0026#34;https\u0026#34; protocol = \u0026#34;HTTPS\u0026#34; backend { group = google_compute_global_network_endpoint_group.default.self_link balancing_mode = \u0026#34;UTILIZATION\u0026#34; capacity_saler = 1.0 } } ","permalink":"//localhost:1313/building-load-balancers-in-gcp-with-terraform/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/building-a-lb-with-tf-logo.png\"\u003e\u003c/p\u003e\n\u003cp\u003eNetworking in GCP, or any cloud environment, often requires a load balancer(LB). If you try to use Terraform to deploy a load balancer in GCP you\u0026rsquo;ll notice that there is not a single Terraform resource to create a GCP LB. GCP provides example Terraform scripts to create LBs in their documentation \u003ca href=\"https://cloud.google.com/load-balancing/docs/load-balancing-overview\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eHowever, if you have an understanding of how load balancers work internally, and how to properly structure your Terraform code, creating your own LB modules in GCP using Terraform is possible. This can allow you to configure your LBs to do exactly what you need.\u003c/p\u003e","title":"BUILDING LOAD BALANCERS IN GCP WITH TERRAFORM"},{"content":" In this post:\nWhat is Ansible\nInstalling Ansible\nAuthentication\nInventories\nPlaybooks\nTesting Ansible\nWhat is Ansible? Ansible is an open-source infrastructure as code tool provided by Red Hat. Instead of agents, Ansible relies on SSH to pass tasks defined in YAML to remote machines. To get familiar with Ansible, I created a simple scenario where a bash script needed to be uploaded to a remote machine and ran periodically.\nIn a previous blog post I\u0026rsquo;ve explained the process of building a Kubernetes cluster using 3 Rasperry Pis in my home computer lab. For this project I used my personal MacBook and one of the Pis from that cluster. Although it is possible to use Ansible with all 3 Pis, for the simplicity of this project I used only 1. Installing Ansible To install Ansible, the Ansible package just needs to be downloaded onto a machine running MacOS or Windows and with Python 3.8 (or a newer version of Python) installed. The machine with Ansible installed becomes the control node, and can then manage a fleet of machines or \u0026ldquo;managed nodes\u0026rdquo;. Ansible usually uses SSH to connect to managed nodes and transfers tasks using SFTP. That being said, if any of the nodes can\u0026rsquo;t use SFTP you can switch to SCP in the Ansible config file ansible.cfg. I have a MacBook, and to install Ansible using pip I ran: sudo python3 -m pip install ansible This installed Ansible my home directory under /Users\nAuthentication Since Ansible connects to managed nodes using SSH, I needed to create an SSH key pair. The private SSH key would remain on the control node, and the public SSH key would be placed on the managed node. This way, the connection between the nodes was secure for SSH.\nNote: To avoid using SSH keys, the --ask-pass argument can be added to Ansible commands to prompt the user for the SSH password.\nTo create an SSH key pair, I used the ssh-keygen command: ssh-keygen -t rsa -f FILE_NAME The key pair is placed in the default SSH key directory, ~/.ssh The public keys have the .pub extension, this is the key that needs to be placed on the managed node in the ~/.ssh/authorized_keys directory. To place my \u0026ldquo;id_rsa.pub\u0026rdquo; public key on the managed node I used an SCP command with the syntax: scp LOCAL_PATH_OF_KEY USER@IP_OF_CONTROL_NODE:~/.ssh/authorized_keys\nInventory Once the control node and the managed node(s) have a secure connection, the managed nodes can be added to an inventory. Ansible inventories specify managed nodes that should be used by Ansible, and you can use an inventory to organize your managed nodes. The default location for Ansible\u0026rsquo;s inventory is /etc/ansible/hosts. The -i PATH_TO_HOSTS_FILE argument can be added to Ansible commands to specify a different path for the inventory if you are working out of a different directory (which I did).\nThe contents of an inventory can be as simple as the IP addresses of the managed nodes, which is what I used. However group names enclosed in brackets [] and assigning aliases to hosts with ALIAS ansible_host=IP can help organize the inventory. Once the managed hosts were defined, I tested the connection between the control and managed nodes using a ping command with the syntax: ansible all -i PATH_TO_HOSTS_FILE -u USERNAME_ON_MANAGED_NODE -m ping Playbooks Next, the tasks to be passed along to the managed nodes defined in the inventory need to be specified in a playbook. Playbooks are YAML files that can include variables and tasks, and are executed from top to bottom. Tasks are named with the -name line, the value of which appears when Ansible executes the playbook so that you can keep track of when tasks run. Like Python, Ansible has a variety of modules that can be called to perform tasks. The list of Ansible modules can be found on their site and incorporated into the YAML of playbooks.\nSimilar to Terraform, Ansible can check whether anything needs to be changed on managed nodes to meet the desired configuration in the playbook. Therefore, if nothing needs to be changed Ansible won\u0026rsquo;t do anything. For my playbook, I needed to upload a bash script to a managed node and have that script run on a schedule. For testing purposes, I created a simple bash script, \u0026ldquo;test-script.sh\u0026rdquo;, to place \u0026ldquo;Hello World\u0026rdquo; into a new file \u0026ldquo;output.sh\u0026rdquo; each time the script ran. In my playbook, playbook.yml, I created two tasks. The first task transferred test-script.sh from the control node to the home directory of the user \u0026ldquo;pi\u0026rdquo; on the managed node. The second task used Ansible\u0026rsquo;s \u0026ldquo;cron\u0026rdquo; module to create a cron job on the managed node. The details of the cron job set test-script.sh to run daily at 2 and 5.\nNote: I used the cron module since the managed node was running Linux. If the managed node is a Windows machine, the Ansible module win_scheduled_task can be used to run tasks on a schedule.\nNote: It is important to give scripts the appropriate permissions to run, hence \u0026ldquo;mode=0777\u0026rdquo;\nTesting Ansible To test the playbook, I ran the command ansible-playbook using the syntax: -i PATH_TO_HOSTS_FILE playbook.yml To confirm that Ansible worked, I SSHed into the managed node and checked that test-script.sh was on the machine, the output.sh file was created and contained \u0026ldquo;Hello World\u0026rdquo;, and that the cronjob specified in the playbook existed using the command crontab -l. I then removed the cron job using crontab -r and deleted test-script.sh and output .sh using the rm FILE_NAME command. ","permalink":"//localhost:1313/ansible/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/ansible-logo.png\"\u003e\n\u003cstrong\u003eIn this post:\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eWhat is Ansible\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eInstalling Ansible\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAuthentication\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eInventories\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePlaybooks\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTesting Ansible\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"what-is-ansible\"\u003eWhat is Ansible?\u003c/h3\u003e\n\u003cp\u003eAnsible is an open-source infrastructure as code tool provided by Red Hat. Instead of agents, Ansible relies on SSH to pass tasks defined in YAML to remote machines.  To get familiar with Ansible, I created a simple scenario where a bash script needed to be uploaded to a remote machine and ran periodically.\u003c/p\u003e","title":"INTRODUCTION TO ANSIBLE"},{"content":" In this post:\nIntro\nJSON File\nCSV File\nPython Script\nWriting the Python Script\nI was recently tasked with automating the replacement of key:value pairs in a JSON file. The goal was to take the output of a CLI command listing key:value pairs and place those keys/values in specific places within a JSON file. I decided to convert the output of the CLI command to a CSV file and have a python script use that CSV to create a new JSON file with updated keys/values.\nThere are a wide variety of python modules that can be used to search and replace values in a file-however the JSON file I was working with presented some challenges. In this post I\u0026rsquo;ll go into detail about the format of that file, the CSV, and how to write a python script to search/update values in JSON.\nJSON File To keep things simple, I have replaced the actual JSON file I used with a fake one. However the format of the file and the location of the values to be replaced are the same. Ideally the python script could automate the replacement of any number of values, but we will only be working with 2 sets of key:value pairs. This project results in 2 JSON files-the original JSON file and a new one with updated values. In my project there were 6 values spread across 3 blocks that needed to be replaced. First 2 old keys together in one block, old-key1 and old-key2 needed to be updated. Further down, both key:value pairs old-key1:old-value1 and old-key2:old-value2 needed to be updated in their own blocks- and it was very important that the keys were matched with their corresponding values. I discovered that the nested format of the JSON file made it difficult to search for values in the file using python. Additionally-some values were of the type list and some were type dictionaries. To find the work around for this, see the \u0026ldquo;Writing the Python Script\u0026rdquo; section.\nCSV File The new values to be placed in the json file came from a CSV file I generated from CLI output. However, the CSV can contain just about anything and use any delimiter. In this case, I used two sets of comma separated values and column headers: Python Script Lines 4-12: Setup\nIn the first portion of the python script, I import the necessary modules (csv and json) and initialize 2 lists and a total of 6 variables. One list will contain the keys from the CSV, the other will contain the values. This is so the content of the CSV can be easily processed in the script. The variable y is set to 1, while the rest are 0-this comes into play later in the script and ensures that the iterations work correctly.\nLines 14-19: Accept input from CSV and add values to lists\nOnce everything is setup, the CSV file must be opened and the keys/values be placed into their respective lists. First, line 14 opens the CSV file \u0026ldquo;input.csv\u0026rdquo; as the variable myfile with read r permission. Then the csv module is used to read the file and specify that the delimiter used is a comma \u0026ldquo;,\u0026rdquo;. Line 16 specifies to skip the first line in the CSV file since mine contains column headers. Finally, lines 17-29 iterate through the rows in the CSV and add the keys to the keys_list and the values to the values_list. This is done by specifying that the keys are in the 0 index of each row, row[0], while the values are in the 1 index row[1].\nLines 22-34: Open original JSON file and update keys in file\nLine 22 opens the JSON file in the same way the CSV was opened, but here the JSON module is used to load the content as read_content. Next, 3 separate for loops are used to replace the keys in the JSON file. The first for loop replaces the keys that are together in a nested block in the JSON file, without being paired with their values. This is the only for loop that uses 2 variables to iterate. a iterates through key_list and since a is set to equal 0 the iteration begins at the first key in the key_list. y, which is set to equal 1, iterates through the original values in the JSON file that are being replaced in this for loop. The original JSON file has 2 values that need to be replaced using this for loop: old-key1 and old-key2. This is specified in line 25 as \u0026quot;old-key\u0026quot;+str(y). So as y is increased by 1 in the for loop the old key is replaced with the corresponding new key. The second and third for loops replace the other keys in the script when they are separated with their respective values. This means that the exact location of the value to be replaced is specified, instead of using an if statement like the first loop. Lines 37-41: Update values in file\nThe values are replaced in the JSON file similarly to how the keys were replaced using for loops. Here 2 loops are used, each replacing one value in the JSON file which correspond to the second and third loops that replace keys. Once this portion of the script is ran, the key:value pairs that are grouped together in the JSON file are replaced with new key:value pairs. Lines 44-45: Write to new JSON file\nFinally, a new JSON file \u0026ldquo;new-file.json\u0026rdquo; is opened as access_json with write w permission. Then the json module combines the content of the old JSON file read_content with the updated content access_json. This creates a new JSON file with the updated values while the original JSON file is untouched.\nWriting the Python Script Creating the python script involved lots of trial and error since, as mentioned previously, the nesting in the JSON file caused the usual methods I use to find/replace values using python to fail. My first thought was to iterate through the JSON file and use an if statement to say if oldvalue: oldvalue=newvalue, newvalue being one of the keys/values from the CSV. However, with that method it seemed that python was only going through the outermost layer of the JSON file. While I\u0026rsquo;m sure there is a \u0026ldquo;better\u0026rdquo; way of doing this-I ended up keeping the if x: x=y to replace the values in the JSON file, but specified the exact location inside the file to find \u0026ldquo;x\u0026rdquo;.\nThis is where the trial and error comes in. For each value that needs to be replaced, I used print statements to narrow down the location of where I was inside the file. The format I used was read_content followed by the names of each nested layer in brackets []. This is similar to using indexes to find values in a list. Some layers of the JSON file contained multiple layers of the same name which meant that using [\u0026quot;name of layer\u0026quot;] wouldn\u0026rsquo;t work. To figure out how to get where I needed, I checked the type of where I was using print(type(read_content['layer1']['layer2']etc)).\nI discovered that layers with multiple layers within them are lists-which meant that I could use the index of the layer I wanted to go into as part of the path I was specifying. For example, read_content ['json']['group2']['resources'][0] to enter one layer and read_content ['json']['group2']['resources'][1] to enter another layer both under json \u0026gt; group2 \u0026gt; resources. NOTE: If you use Visual Studio Code to open the JSON file, you can see the path to a value at the top of the screen if you click on a value. You can use this path separated by brackets instead of using print statements to find the path. ","permalink":"//localhost:1313/replace-json-values/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/json-logo2.jpeg\"\u003e\n\u003cstrong\u003eIn this post:\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eIntro\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eJSON File\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCSV File\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePython Script\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eWriting the Python Script\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eI was recently tasked with automating the replacement of key:value pairs in a JSON file. The goal was to take the output of a CLI command listing key:value pairs and place those keys/values in specific places within a JSON file. I decided to convert the output of the CLI command to a CSV file and have a python script use that CSV to create a new JSON file with updated keys/values.\u003c/p\u003e","title":"AUTOMATE REPLACING VALUES IN JSON USING PYTHON"},{"content":"\nUntil now, the blog I use to document my projects has been hosted on WordPress. To cut down on the cost of running my blog (and to complete a cool project), I’ve made my blog a static website.\nMost websites we visit are dynamic websites, meaning that a web server generates the site with content that can change and be interacted with by users. Static websites are the opposite, they display the same content for every user and are shown exactly as they are written (usually using HTML).\nCreating your static website does involve some coding, but all-in-all it can be easy. I used Hugo, Git, AWS Amplify, and AWS Route53 to create my static website.\nHugo Hugo is an open-source static website generator. In a nutshell, you install Hugo on your machine and use it to create static website folders. You can select a website theme from Hugo and then add your posts to the static website’s folder on your computer. Hugo can be run with a few simple CLI commands, but you must also write your posts in the markdown language that Hugo uses.\nI used homebrew to install hugo on my MacBook, but you can find other ways to install Hugo on their website\nbrew install hugo\nTo check that hugo has successfully installed, run hugo version in your terminal. Now that hugo is installed, you can create a static website using hugo new site nameofsite.\nThis will create a variety of folders for your static webpage. Before you begin adding content to your site, you must install a theme. There are a variety of themes to choose from on Hugo’s webpage but the more complex the theme, the more difficult it is to configure your site.\nThe location of where you need to upload your posts/images varies depending on what theme you use. This can make getting your site up and running a little difficult. Themes do come with an “exampleSite” folder that you can reference for how to set up the folders of your static site, but there is some trial-and-error when creating your posts to see what loads correctly.\nTo install your theme, Hugo recommends using Git-but for simplicity’s sake I navigated to my theme of choice (https://themes.gohugo.io/ghostwriter/) on Hugo’s website and downloaded a .zip file of the theme. I then unzipped that theme in the /theme folder of my static website.\nOnce your theme is installed, navigate to that theme’s folder and find the /exampleSite folder. From there-copy any folders under the example site’s /content folder you would like to use into your site’s /content folder. I wanted to use /page and /post. You’ll notice there is also a sample config file in the exampleSite. You can copy the content of the example config file into your site’s config.toml to add your theme’s information to your site.\nNOTE: My theme was originally unzipped as “ghostwriter-master” in the /theme folder, but after copying the example config into my site’s real one- I saw that the theme = “ghostwriter”. To make the config file find my theme, I renamed the theme folder from “ghostwriter-master” to “ghostwriter”.\nFurther editing the config.toml file allows you to change the title, heading, and other options on your site.\nTo test your static website locally, run hugo server -D in the root directory of your website then navigate to (http://localhost:1313/) in your browser. This allows you to see what your website will look like before you actually deploy it.\nTo deploy your website run hugo in the root directory of your website. This generates a /public folder with all the necessary files for your static web page-which you will need later in the process of setting up your static website.\nWordPress to Hugo Once you have created the folders for your static website with Hugo and set up a theme, you can begin adding posts. Since I was moving my blog from WordPress, I needed to transfer my existing posts from WordPress to Hugo to avoid having to rewrite all my posts from scratch.\nI used the ExitWP extension to convert my existing WordPress posts to markdown for me. First, download the extension using git clone https://github.com/wooni005/exitwp-for-hugo.git which creates a variety of folders. Next, on WordPress, I navigated to the export tool and chose to download just the posts.\nThe posts export as .xml files, which I placed into the wordpress-xml folder the ExitWP extension created. A handful of dependencies are required to run the ExitWP extension. The commands to download these dependencies can be found on the extension’s Git page.\nOn my Mac I ran: sudo pip install \u0026ndash;upgrade -r pip_requirements.txt\nOnce the dependencies are installed, run ./exitwp.py in the root directory of the extension to execute the python script that converts the WordPress posts to markdown. The converted files should show up in the extension’s /build folder.\nWriting Posts Although I converted posts from WordPress to markdown, a little tweaking on the converted posts was necessary. For example, I changed the extension of the posts to be “.md” instead of “.markdown” to match the post format from the example site. I also had to download all the photos in my posts from WordPress and store them in the /static folder of my site.\nNOTE: The example site of my theme had images stored in an /image folder under the main directory, but I discovered they only showed up on my site when I stored them in the /static folder instead-once again trial and error.\nOnce my photos were downloaded into the correct folder, I had to open each post file and change the format of the image elements to point to the correct photo. For my blog the syntax was ![alternate text] (/photo name).\nWhen writing new posts using Hugo, you have to use markdown language to format your posts correctly. The syntax for markdown can be found on Hugo’s website. and it\u0026rsquo;s pretty similar to writing in HTML.\nYou define the format of your post (like italics, bulleted lists, etc.) using markdown, but the actual style of your post and website is defined by your theme. You can edit that style by editing the config.toml file or the config files in your theme folder, but I chose to get the site working before making any style changes.\nGit Once the Hugo static website is set up on your machine, the next step is making the site available to the public. To do this, I set up a Git repository of the /public folder my Hugo site generated. I then used AWS Amplify to point to that repository and serve the website using an AWS Route 53 custom domain name.\nWhile you can place all the static site’s folders in the repo, the /public folder stores all the files for the public version of the website and adding anything more would be unnecessary.\nI have Git and the GitHub Desktop app installed on my machine. First I navigated to the /public folder of my website in the terminal and ran git init name_of_repo and then git add .\nThese commands initialized a repo for the /public folder (and gave that repo a name) then added all the files in that folder to the repo. Now that the files are staged to be committed to the repository, I ran git commit and then git push.\nAt this point, I navigated to my GitHub account and confirmed that the git repository was there with all the files under the site’s /public folder.\nAWS Amplify Once the /public folder of the static website was placed into a public Git repo, I used AWS Amplify to host the website.\nAWS Amplify allows you to host apps in the cloud using Amazon Web Services. While Amplify does offer a variety of complex services for things like machine learning and targeted campaigns, creating a static website using Amplify is pretty simple.\nYou must have an AWS account to use Amplify and Route 53 (discussed later). Once you log in to your AWS account navigate to the “AWS Amplify” service.\nIn Amplify, select “New app” \u0026gt; “Host web app” Select GitHub under “From your existing code” since you will be connecting the Git repo with the /public folder in it to Amplify. From there, you can select the repo you would like to connect to Amplify and the branch (in my case, it was just the main branch since I did not add more branches to the repo). Next, you can change the app name and the app’s settings which I kept the same. Finally, you review the contents of your app and then select “Deploy” to create it.\nNow in the AWS Amplify service, you should see your website as an app under “all apps”. If you click on your app you can watch the process as Amplify provisions, builds, deploys, and verifies it for you.\nYou can click on the link Amplify provides you under the picture of your app to visit your static webpage and make sure it looks like it should.\nRoute 53 The last step I took in configuring my static website was to add a custom domain name to my Amplify app using Route 53. The instructions to do so can be found here.\nRoute 53 is a Domain Name System (DNS) service AWS provides. You can use Route 53 to route end users to your apps using custom, or “prettier” domain names.\nTo use Route 53, first navigate to the Route 53 service in AWS. From there, you can view your registered domains. If you don’t have any, you can select the “Register Domain” button to do so-but the domain for my blog (osnowden.com) had already been registered.\nBack in Amplify, select App Settings \u0026gt; Domain Management\nThen choose to “Add domain” and fill out your domain’s information. When I added my domain, it became stuck on the “Domain activation” step.\nTo fix this, I had to add two records to my domain in Route 53 so that it would resolve the Amplify app correctly. The instructions to do so can be found under the Actions \u0026gt; View DNS Records button on Amplify’s “Domain management” page.\nTo add the records, I navigated to Route 53 \u0026gt; “Hosted zones” and selected my domain to view it’s records. Selecting “Create records” allowed me to then add the two necessary records.\nI first added an A record with the format @.osnowden.com A appid.cloudfront.net and a CNAME record with the format www.osnowden.com CNAME appid.cloudfront.net\nThe appid of my Amplify app I copied from the URL at the bottom of the “Domain management” page in Amplify. The URL should be in the format: https://master.appid.amplifyapp.com\nOnce the records were added, my static website was then accessible from www.osnowden.com!\n","permalink":"//localhost:1313/static-blog/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/static13.png\"\u003e\u003c/p\u003e\n\u003cp\u003eUntil now, the blog I use to document my projects has been hosted on WordPress. To cut down on the cost of running my blog (and to complete a cool project), I’ve made my blog a static website.\u003c/p\u003e\n\u003cp\u003eMost websites we visit are dynamic websites, meaning that a web server generates the site with content that can change and be interacted with by users. Static websites are the opposite, they display the same content for every user and are shown exactly as they are written (usually using HTML).\u003c/p\u003e","title":"STATIC BLOG WITH HUGO, GIT, \u0026 AWS"},{"content":"Once you get past your first \u0026ldquo;Hello World\u0026rdquo; Python script, it\u0026rsquo;s good to learn how to use Python methods and classes. Methods and **functions **are a self-contained block of code that can be reused over and over. There are built-in functions that beginners in Python may already be familiar with, like print() and sum(). A user can also define a function (called a user-defined function) that executes a task they need done in their script multiple times. This is especially useful because it keeps the user from having to write that same chunk of code repeatedly. Variables that exist outside of the function can be passed to the function as parameters, but they are not required. The format of defining your own function is below:\n\u0026lt;code\u0026gt;def function_name(parameters): body_of_code return variable_to_be_returned\u0026lt;/code\u0026gt; The return statement of a user-defined function basically provides the \u0026ldquo;output\u0026rdquo; of that function. Since functions are self-contained, if you want a variable returned from what task the function carried out you need to specify that variable in a return statement. Functions return a value or a group of values, while methods generally do not.\nPython is an object oriented language, meaning that you can model real-world things through programming. An object represents an entity in the real world that has a unique state, identity, and behaviors (such as a phone, a button, or a person). Classes define objects of the same type. For example, a type of class could be a car. However, each individual car would be an object with their own characteristics (color, fuel type, etc.).\nTo create an object in a class you use constructors, which are a type of method. In Python the constructor is init(self). This is the format to define a class:\n\u0026lt;code\u0026gt;class name_of_class: def ___init___(self): class_body\u0026lt;/code\u0026gt; Class, objects, and constructors can be difficult to wrap your mind around initially, but they are incredibly useful. To help explain, below is an example that creates unique Python User accounts for an organization:\nThe goal is to create an object-oriented class (PythonUsers) for generating new users. This class should have the following attributes: first_name, last_name, age, username and current password_. When defining the class, it should a method that generates a password for the user based on the first and last initials followed by the current time stamp.\nFirst, we define the PythonUsers class, and in the constructor method init() we include the variables each user object will need (including the \u0026ldquo;self\u0026rdquo; keyword, it is required). Inside the class, we call the constructor method to assign each of those variables to become a class variable. Notice that the password variable is not included here, since a separate method will create the passwords.\n\u0026lt;code\u0026gt;#PythonUsers class class PythonUsers: def __init__(self, first_name, last_name, age, username): self.firstName = first_name self.lastName = last_name self.age = age self.username = username\u0026lt;/code\u0026gt; Once the basic setup of the class is finished, we define a method that generates the password. We import the time module so that we can include the time stamps, and then compile the password using the index of the first letters of the user\u0026rsquo;s first/last names (0) and the time module mentioned previously. Notice that since we are still defining an element of the class, the \u0026ldquo;self\u0026rdquo; keyword is used in the password method.\n\u0026lt;code\u0026gt;#Method that generates a password for the user def passwordGen(self): import time ts = time.time() password = self.firstName[0] + self.lastName[0] + str(ts) self.password = password\u0026lt;/code\u0026gt; Now that we have a class defined to create users, we need a method (calcAverageAge) that accepts a list of users as a parameter and returns the average of their ages.\n\u0026lt;code\u0026gt;#Method that calculates average age of the students def calcAverageAge(sum, n): average=sum/((n + 1) - 1) return average\u0026lt;/code\u0026gt; Next, we need a method (validateAge) that validates the age of a user. For the purposes of this example, assume a user should be at least 15 years old and not more than 45 years old. If a user enters a date of birth that does not meet the above requirement, the method should keep prompting the user until a valid age is entered.\n\u0026lt;code\u0026gt;#Method that validates user's age def validateAge(age): while age \u0026gt; 45 or age \u0026lt; 15: age = int(input(\u0026quot;Please enter the age of user \u0026quot; + str(i + 1) + \u0026quot; [15-45]: \u0026quot;)) return age\u0026lt;/code\u0026gt; Using the class and methods implemented above, we can complete our script. For each user, we should request for the user to provide the first_name, last_name, username and age. Remember we need to validate each users age (using the**_ validateAge _**method). We then print to the screen the list of users including their full name, age and default password. Once the users are created and validated, we can we can compute the average age of users using the **_calcAverage _**method and print that out.\n\u0026lt;code\u0026gt;#Main program that accepts integer from user and creates a Python user sum=0 userList=[] n=int(input(\u0026quot;Please enter the number of users: \u0026quot;)) for i in range(n): first_name=input(\u0026quot;Please enter the name of user \u0026quot; + str(i+1) + \u0026quot;: \u0026quot;) last_name=input(\u0026quot;Please enter the last name of user \u0026quot; + str(i+1) + \u0026quot;: \u0026quot;) username=input(\u0026quot;Please enter the username of user \u0026quot; + str(i+1) + \u0026quot;: \u0026quot;) age=int(input(\u0026quot;Please enter the age of user \u0026quot; + str(i+1) + \u0026quot; [15-45]: \u0026quot;)) #Validate the user's age using validateAge age = validateAge(age) sum = age + sum object=PythonUser(first_name, last_name, age, username) object.passwordGen() userList.append(object) #Print name, age, and password - outside the loop, separate loop for printing for i in range(len(userList)): fullname = userList[i].firstName + \u0026quot;,\u0026quot; + userList[i].lastName age = userList[i].age newpassword= userList[i].password print(fullname + \u0026quot;\\t\u0026quot; + str(age) + \u0026quot;\\t\u0026quot; + str(newpassword)) #Compute average age of user with calcAverage avg = calcAverageAge(sum, n) print(\u0026quot;Average Age of users: \u0026quot; + str(avg))\u0026lt;/code\u0026gt; Here we used methods and classes to generate users, but the same concepts can be applied to a variety of situations where a script can be used to automate a task. Overall, methods/functions and classes are useful aspects of Python that can make writing a script easier-and save you a lot of typing.\n","permalink":"//localhost:1313/python-methods/","summary":"\u003cp\u003eOnce you get past your first \u0026ldquo;Hello World\u0026rdquo; Python script, it\u0026rsquo;s good to learn how to use Python methods and classes. \u003cstrong\u003eMethods\u003c/strong\u003e and **functions **are a self-contained block of code that can be reused over and over. There are built-in functions that beginners in Python may already be familiar with, like print() and sum(). A user can also define a function (called a user-defined function) that executes a task they need done in their script multiple times. This is especially useful because it keeps the user from having to write that same chunk of code repeatedly. Variables that exist outside of the function can be passed to the function as \u003cstrong\u003eparameters\u003c/strong\u003e, but they are not required. The format of defining your own function is below:\u003c/p\u003e","title":"PYTHON METHODS \u0026 CLASSES"},{"content":"Over the next year, I will be completing a research project exploring how biased training data effects the machine learning algorithm of self-driving cars. This project, which I\u0026rsquo;ll detail in a later post, involves deep neural networks (DNNs), computer vision, linear algebra, and more. Is this project a massive undertaking? Yes. Will that stop me? Absolutely not. Let\u0026rsquo;s get started.\nNeural Network Basics First, what is artificial intelligence (AI)? AI is a field of computer science pertaining to programming computers so that they demonstrate human-like intelligence.\nMachine learning (ML) is a subset of AI that allows algorithms to learn by working on training data before being exposed to real-world problems. Furthermore,** neural networks** (NNs) are a subset of ML that mimics the human brain to make decisions. Most NNs are multi-layered networks that consist of 1 input layer, 1 output layer, and 1+ hidden layers in between. If there are many hidden layers, the NN is considered a** deep neural network **(DNN).\nThe layers in a NN are made up of neurons that, in short, take in a value and spit out a value. Each neuron in one layer is connected to all the neurons in the next layer. The number of input neurons is the number of features that the NN will work with. In turn, the number of output neurons is the number of predictions that you want the NN to make. The value that a neuron holds is its activation. The activation function of a neuron is an equation that determines whether the neuron should be activated, AKA \u0026ldquo;fired\u0026rdquo;, or not. They also help normalize the output of the neurons to be in a small range, usually between 1 and 0 or -1 and 1.\nWeights and biases are the parameters that allow a NN to \u0026ldquo;learn\u0026rdquo;. Weight affects how much the input will influence the output - how strong the connections between two neurons are. Bias, which stores the value 1, are often added to each layer in a NN to move the activation function left or right. It represents the difference between the function\u0026rsquo;s output and it\u0026rsquo;s intended output. Bias serves as a constant that allows the NN to become more flexible and helps control when the activation function will trigger.\n**Cost functions **help the network learn which changes matter most and reflects how the network is performing. The cost is small when the network is confident, but large when it isn\u0026rsquo;t sure what it\u0026rsquo;s doing. The NN learns by finding the right weights and biases to minimize that cost, this is called gradient descent. Applying gradient descent to the cost function allows the NN to find weights that result in lower error and makes the NN more accurate over time.\nBackpropagation is the process of nudging the weights and biases to decrease cost working from the output layer backwards-after the NN has worked through data. A DNN can loop through its layers using backpropagation: data enters the network through the input layer, is worked on in the hidden layers, and exits through the output layer only to be fed back into the network with updated weights and biases to improve performance.\nTensorFlow Basics TensorFlow is a free software for machine learning, including pre-trained models, datasets, and tutorials. A tensor is a type of data structure, like a vector or matrix, that is basically a multidimensional array.\nTensors come in handy when developing ML systems, because they allow you to process large quantities of multi-dimensional data. For example, and image can be defined using three features: hight, width, and color(depth). If you are trying to process images in a NN, like I am, a tensor allows those features to be considered.\nKeras is the high-level API used by TensorFlow that provides a Python interface for NNs. To get started with TensorFlow I used Google Colab, which allows you to write and execute python in your browser. Colab is especially useful for people like myself, who work from a small laptop that wouldn\u0026rsquo;t be able to handle the compute power required when working with DNNs.\nNeural Net with TensorFlow-Example The first NN that I created using TensorFlow is basically the \u0026ldquo;Hello World\u0026rdquo; of NNs. It is a pretty common example you\u0026rsquo;ll see if you research TensorFlow tutorials. Using the MNIST handwritten digit database, I trained a NN to recognize images of numbers using Python:\nFirst I imported tensorflow, keras, layers (to create the layers of the NN), and the MNSIT dataset:\n\u0026lt;code\u0026gt;import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.datasets import mnist\u0026lt;/code\u0026gt; Then, I divided the MNIST dataset into training data and testing data, to test/train the NN respectively. Notice that the data is split into x and y values, like a graph. When categorizing an image ML algorithms will view data in a 3- D space and will draw lines between like data points clustered together (hyperplanes) to classify them. Therefore, we need to work with data in that type of space. Since I was working with images I reshaped them to ensure they were all the same size, 28 pixels x 28 pixels, and normalized the data by dividing by 255.\n\u0026lt;code\u0026gt;(x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(-1, 28 * 28).astype(\u0026quot;float32\u0026quot;) / 255.0 x_test = x_test.reshape(-1, 28 * 28).astype(\u0026quot;float32\u0026quot;) / 255.0\u0026lt;/code\u0026gt; Next I defined the NN to use. Utilizing Keras, I chose a sequential model that is useful when each layer has one input tensor and one output tensor. Sequential models are most likely not the best to classify images, but for the sake of this example it was a simple model to use. To add layers to the NN, I used model.add() and first defined the number of neurons and then the activation function for each layer. I used the relu, or Rectified Linear Unit, activation function which is efficient and allows for backpropogation. Below, line two is the input layer, lines 3-4 are the hidden layers, and line 5 is the output layer.\n\u0026lt;code\u0026gt;model = keras.Sequential() model.add(keras.Input(shape=(784))) model.add(layers.Dense(512, activation=\u0026quot;relu\u0026quot;)) model.add(layers.Dense(256, activation=\u0026quot;relu\u0026quot;, name=\u0026quot;my_layer\u0026quot;)) model.add(layers.Dense(10))\u0026lt;/code\u0026gt; I then specified the training configuration. I defined the loss function, which calculates the error of the NN. I chose the Sparce Categorical Crossentropy (SCCE) loss function, which means that each sample can belong to only one class. I also used the Adam (Adaptive Momentum Estimation) optimizer, which works to find learning rates for each parameter to make the NN more efficient. To track the accuracy of the NN as it learns, I added an accuracy metric.\n\u0026lt;code\u0026gt;model.compile( loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False), optimizer=keras.optimizers.Adam(lr=0.001), metrics=[\u0026quot;accuracy\u0026quot;], )\u0026lt;/code\u0026gt; I used the model.fit() and model.evaluate() APIs built into TensorFlow to test and train my NN. model.fit() trained my model by slicing data into batches and iterating over the entire dataset for a set number of repetitions, called epochs. model.evaluate() tested the model using my testing data. The verbose option provided output to my screen as the script ran.\n\u0026lt;code\u0026gt;model.fit(x_train, y_train, batch_size=32, epochs=5, verbose=2) model.evaluate(x_test, y_test, batch_size=32, verbose=2)\u0026lt;/code\u0026gt; Adding more epochs, changing the size and number of layers, and editing the training configuration will help improve the performance of this NN. After this basic NN, working with larger datasets and DNNs is the next step.\n","permalink":"//localhost:1313/capstone-pt1/","summary":"\u003cp\u003eOver the next year, I will be completing a research project exploring how biased training data effects the machine learning algorithm of self-driving cars. This project, which I\u0026rsquo;ll detail in a later post, involves deep neural networks (DNNs), computer vision, linear algebra, and more. Is this project a massive undertaking? Yes. Will that stop me? Absolutely not. Let\u0026rsquo;s get started.\u003c/p\u003e\n\u003ch4 id=\"neural-network-basics\"\u003eNeural Network Basics\u003c/h4\u003e\n\u003cp\u003eFirst, what is \u003cstrong\u003eartificial intelligence\u003c/strong\u003e (AI)? AI is a field of computer science pertaining to programming computers so that they demonstrate human-like intelligence.\u003c/p\u003e","title":"NEURAL NETWORKS \u0026 TENSORFLOW - Capstone Pt.1"},{"content":"In my first Capstone post I gave a rundown of basic AI terms and how to use Tensorflow to create your own machine learning (ML) script.\nI\u0026rsquo;m using Tensorflow to write a script that can process images of roads and determine whether there is an obstacle in the road or not. Since I\u0026rsquo;m studying self-driving cars, I\u0026rsquo;d like to see if using biased data to train an object detection script affects the model\u0026rsquo;s performance. To do this, I\u0026rsquo;m using a training dataset of clear, bright road images to train a neural network and then testing that script with corrupted images of roads. If the model can\u0026rsquo;t recognize obstacles in images that are corrupted, then the biased training dataset did have an affect on the model.\nIts safe to assume that yes-using biased datasets to train a neural network (NN) will cause the network to function poorly. Its important for NNs to be trained with large, diverse datasets so that they can prepare for unpredictable real-world scenarios. This point has been made in almost any piece of work pertaining to NN. That being said, I\u0026rsquo;d like to use Tensorflow to create a NN and then test the affects of bias myself. Tensorflow allows for testing and training accuracy of a model to be quantified easily, and you can use python modules to make nifty little graphs that show results.\nIts also important to mention that the software of real self-driving cars, like those from Waymo or Audi, are not opensource (no surprise there). So any model created by a novice Tensorflow user like myself is a far cry from the object-detection systems in actual self-driving cars. That being said, the point here is to test how biased training data could affect an object detection system and practice using Tensorflow along the way.\nScript Breakdown The object detection script I wrote using Tensorflow consisted of two datasets (one to train the model and one to test the model), a convolutional neural network (CNN), and a diagram to display the accuracy of the model. The sections of the script are:\nimport modules create training dataset create testing dataset standardize the data create + compile the model data augmentation model summary train the model visualize results More robust object detection scripts often use bounding boxes (little boxes that surround an item and usually name it) to recognize unique objects in an image. However, my script doesn\u0026rsquo;t implement bounding boxes and instead classifies images as either having an obstacle, or not.\nImport Modules + Datasets Since I\u0026rsquo;m using python to write my script, the first step is to import all the modules needed:\nimport os import PIL import PIL.Image import tensorflow as tf import numpy as np from tensorflow import keras import matplotlib.pyplot as plt from tensorflow.keras import layers from tensorflow.keras.models import Sequential\nWith that out of the way, I next create my datasets. There are a few ways to approach datasets for your own ML script. Here I will only be discussing datasets of images since thats what I\u0026rsquo;m familiar with, but other types of datasets do exist. I ended up using Kaggle to write my script, which allows files to be imported from your computer and added to a dataset. The path to that dataset is stored in Kaggle, and can be copied/pasted into your script to be accessed. You can also use Tensorflow\u0026rsquo;s Google Storage Bucket option to store and reference a dataset, or you can use one of Tensorflow\u0026rsquo;s own datasets, among other options.\nIn most ML scripts, you only use one dataset, and use a percentage of it to train the model and the leftover percentage to test (typically 80%train/20%test). If you train and test a model on the same dataset without it splitting into training/testing sections, accuracy metrics will say the model is 100% accurate since you aren\u0026rsquo;t testing with new data (I learned this the hard way). For my model however, I wanted one testing dataset and one training dataset that could be corrupted later on.\nTo create the testing and training datasets I used Tensorflow\u0026rsquo;s \u0026ldquo;image_dataset_from_directory\u0026rdquo; option. I first set the image\u0026rsquo;s height and width so they would all be a uniform size. I also set the batch size, which will I\u0026rsquo;ll explain later:\nbatch_size = 32 img_height = 180 img_width = 180\nWhen creating the datasets, you first specify the location of your dataset (in this case, the path that Kaggle provided me). Then, seed provides a random seed for shuffling and transformation. The validation_split represents the percentage of data you want to use to train the model and test the model. Here, a validation_split value of 0.2 means that 80% of the training dataset will be used to train the model and 20% of the testing dataset will be used to test (at this point in time the corrupt images have not been added to the testing dataset, so the two datasets are identical and the split should be used). Finally I then set the subset name, the image sizes, and the batch size.\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory( \u0026quot;../input/training-dataset\u0026quot;, seed=123, validation_split=0.20, subset=\u0026quot;training\u0026quot;, image_size=(img_height, img_width), batch_size=batch_size)\nval_ds = tf.keras.preprocessing.image_dataset_from_directory( \u0026quot;../input/testing-dataset\u0026quot;, seed=123, subset=\u0026quot;validation\u0026quot;, validation_split=0.20, image_size=(img_height, img_width), batch_size=batch_size)\nOne thing to consider when using data to train a NN is how the model will learn \u0026ldquo;what is what\u0026rdquo;. An untrained model can\u0026rsquo;t make its own conclusions, and piping random images into a model won\u0026rsquo;t teach it anything. Data should be split into classes of whatever you want to model to learn to differentiate. In my case, I had two classes: \u0026ldquo;obstacle\u0026rdquo; and \u0026ldquo;noobstacle\u0026rdquo;. In my datasets I included two folders, one for each class, and inside each folder I placed the respective images. I included a line in my script to double-check the classes were being read-in correctly:\nclass_names = train_ds.class_names print(class_names)\nStandardize, Creating and Compiling the Model Now that the datasets and classes of data are working, the model itself can be created. Like in my first Capstone post, I will be using the Sequential model, relu activation function, and Adam optimizer. However, for this script I have created a convolutional neural network (CNN).\nA CNN is a class of deep neural networks, and are commonly used to process images since they resemble the organization of an animal\u0026rsquo;s visual cortex. CNNs take in tensors of shapes, in this case the image height, width, and an RGB value of 3 (color). These aspects are then assigned importance through weights and biases in order for the model to differentiate one from another.\nIn Tensorflow, you create a NN through Conv2D and MaxPooling2D layers that each output a tensor of shape. The densest (largest) layers are placed at the top, and the layers shrink as you go down. The number of output channels for each Conv2D layer is controlled by the first argument of that layer. At the end of the model, Flatten and dense layers perform classification by \u0026ldquo;unrolling\u0026rdquo; the 3D images into 1D and using what the NN has learned to classify.\nTo create the CNN the data should first be normalized. This has to do with the mathematics of ML, but this normalization layer basically helps standardize the data.\nnormalization_layer = layers.experimental.preprocessing.Rescaling(1./255)\nNext, I defined the model layers itself:\nnum_classes = 2\nmodel = Sequential([ layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)), layers.Conv2D(16, 3, padding='same', activation='relu'), layers.MaxPooling2D(), layers.Conv2D(32, 3, padding='same', activation='relu'), layers.MaxPooling2D(), layers.Conv2D(64, 3, padding='same', activation='relu'), layers.MaxPooling2D(), layers.Dropout(0.2), layers.Flatten(), layers.Dense(128, activation='relu'), layers.Dense(num_classes) ])\nIn my CNN I added a Dropout layer. This is one of the ways I improved the performance of my model. A Dropout layer randomly sets input to 0 at each layer while training. The inputs that aren\u0026rsquo;t set to 0 are scaled up by 1. This helps prevent overfitting, which occurs when a model conforms too tightly to training data and doesn\u0026rsquo;t learn enough to make predictions on new data.\nmodel.summary() helps to visualize what the layers of the CNN look like:\nTo compile the model I used model.compile:\nmodel.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\nIn addition to a Dropout layer, I also used data augmentation to reduce overfitting. Data augmentation makes multiple different images from one training image, by flipping or zooming in on different sections of the image. This helps add in more training data without having to manually add pictures to my datasets.\ndata_augmentation = keras.Sequential( [ layers.experimental.preprocessing.RandomFlip(\u0026quot;horizontal\u0026quot;, input_shape=(img_height, img_width, 3)), layers.experimental.preprocessing.RandomRotation(0.1), layers.experimental.preprocessing.RandomZoom(0.1), ] )\nTrain and Visualize Training and testing your model with Tensorflow can be done in a few lines of code:\nepochs=27 history = model.fit( train_ds, validation_data=val_ds, epochs=epochs )\nYou\u0026rsquo;ll notice that here epochs are set to 27. I explained back/forward propagation in my first Capstone post, but epochs are basically the number of times your NN loops through its layers to learn. Batch size, which we set earlier, defines how much of your dataset is put into your model at a time. This is a good way to save computation resources, so you aren\u0026rsquo;t trying to put a dataset of potentially millions of images into your model at a time.\nHowever, if you set the number of epochs too low, the NN won\u0026rsquo;t learn enough. If there are too many epochs, the NN can become too familiar with only the training data and overfit when tested. Unfortunately, there is no easy formula to determine how many epochs to use. I just monitored the performance of my NN with different numbers of epochs, and settled on 27.\nWhat can help you determine the number of epochs to use, along with check your model\u0026rsquo;s accuracy, is visualizing your results. In my script I used matplotlib.pyplot to make a graph.\nacc = history.history['accuracy'] val_acc = history.history['val_accuracy']\nloss = history.history['loss'] val_loss = history.history['val_loss']\nepochs_range = range(epochs)\nplt.figure(figsize=(8, 8)) plt.subplot(1, 2, 1) plt.plot(epochs_range, acc, label='Training Accuracy') plt.plot(epochs_range, val_acc, label='Testing Accuracy') plt.legend(loc='lower right') plt.title('Training and Testing Accuracy')\nplt.subplot(1, 2, 2) plt.plot(epochs_range, loss, label='Training Loss') plt.plot(epochs_range, val_loss, label='Testing Loss') plt.legend(loc='upper right') plt.title('Training and Testing Loss') plt.show()\nWrapping it Up In this case, the two most important metrics are the training accuracy and the testing accuracy. The training accuracy for my model was nearly 100%, while the testing accuracy was 72-80% (this varied between runs of my script). To put it into perspective, 50% accuracy would be guessing-since the model has only two options to choose from (obstacle or no obstacle). I can live with 72-80% accuracy, although the large difference between my training and testing accuracy means that my model is overfitting. To help fight overfitting, I most likely need to add more images to my dataset (most image datasets have 10,000+ images, mine have 200) or change the type of activation function/optimizer I am using.\nFrom the graph, you can also see that the training loss (prediction error) drastically lowers over epochs, while testing loss doesn\u0026rsquo;t significantly lower. This is another result of overfitting, although my model does work much better with the inclusion of data augmentation and dropout.\nThe next step is to edit my model slightly to be more robust, then begin corrupting testing images and experimenting. For now, I have a moderately accurate object detection CNN that would almost certainly cause a real self-driving car to crash, but it works for me.\nThe full script can be found on my Github or on Kaggle.\n","permalink":"//localhost:1313/capstone-pt2/","summary":"\u003cp\u003eIn my first Capstone post I gave a rundown of basic AI terms and how to use Tensorflow to create your own machine learning (ML) script.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;m using Tensorflow to write a script that can process images of roads and determine whether there is an obstacle in the road or not. Since I\u0026rsquo;m studying self-driving cars, I\u0026rsquo;d like to see if using biased data to train an object detection script affects the model\u0026rsquo;s performance. To do this, I\u0026rsquo;m using a training dataset of clear, bright road images to train a neural network and then testing that script with corrupted images of roads. If the model can\u0026rsquo;t recognize obstacles in images that are corrupted, then the biased training dataset did have an affect on the model.\u003c/p\u003e","title":"NEURAL NETWORKS \u0026 TENSORFLOW - Capstone Pt.2"},{"content":"Bash, or the Bourne-Again SHell, is named after the creator of the Unix shell Stephen Bourn. Bash is a command language interpreter that can execute commands and process text files as input. Shell scripts are, basically, files containing a series of commands. You can schedule Bash scripts to run at certain times to automate tasks for you.\nIf you are using a Linux machine, you can write and execute Bash scripts in terminal. Through the terminal you can use your preferred text editor (nano, vim, etc.) to write your script and save it with the extension .sh. If you are using a MacBook, like me, you can use the terminal or Visual Studio Code to write Bash scripts- just save them with the .bash extension.\nI love Linux, and I love Bash scripting, but it can be hard to learn. Here are some basics.\nSet-up, Execution, and Redirection The Bash shell scripting language allows for loops, selection statements, input/output, and functions like other programming languages. Each shell script starts with a comment that states which interpreter to use to run the script. For Bash, begin each file with the shebang: #!/bin/bash.\nTo execute a Bash script, you must first change the permissions of the file using chmod 755 _script_name_. Then, you can execute a script by running ./script_name on the command line or by hitting the play button in Visual Studio Code.\nSometimes it can be useful to redirect Bash input/output. script_name\u0026gt; contentbasename will redirect output to a file, script_name \u0026gt;\u0026gt; contentbasename will append the output of the bash script to an existing file, and script_name\u0026lt; contentbasename redirects a file as input to a script.\nVariables and Math The body of a Bash script can consist of variables, commands, and arithmetic operations. Variables are a basic building block of Bash scripts. VARIABLE=VALUE is the basic assignment statement, notice there are no spaces. To call a variable in a Bash script, use $VARIABLE. The \u0026ldquo;print statement\u0026rdquo; for Bash scripting is echo. If you want to output something to the user use echo \u0026quot;statement\u0026quot;/$VARIABLE.\nIf you want to assign an interpreted value with spaces to a variable, you must include double quotes around the value. If you want to interpret the value with spaces literally, use single quotes. Below is an example of a Bash script and its output that shows the difference between single and double quotes.\nBash interprets everything as strings, meaning that if you want to do math-you need to explicitly tell Bash. To do that, include an arithmetic operation in $(()) or place let before it. Bash can\u0026rsquo;t perform arithmetic on decimals or fractions. Below is a list of arithmetic operators bash can process:\nRead and Parameters Getting input from users in Bash is a little different. Anywhere that you want to insert a user\u0026rsquo;s input into a Bash script place $1, $2, $3, etc. Input is accepted as parameters, and the parameters are placed in those spots in the order they are given. If the number of parameters given exceeds the number of spots for them in the script, the extra parameters are ignored.\nIf you want to provide the user a prompt, and assign their parameters names, you can use the read statement in your script: read -p \u0026quot;message\u0026quot; var1 var 2 ...\nSometimes, you want to use metadata about parameters in your script. Bellow is a list of helpful symbols that you can use in your scripts:\n$parameter: calls a parameter shift: rotates all parameters down one position, used in loops $0: returns the name of the script $$: returns the PID of the script $#: returns the number of parameters the user supplied $@: returns a list of all parameters Example script:\n","permalink":"//localhost:1313/bash-basics/","summary":"\u003cp\u003eBash, or the Bourne-Again SHell, is named after the creator of the Unix shell Stephen Bourn. Bash is a command language interpreter that can execute commands and process text files as input. Shell scripts are, basically, files containing a series of commands. You can schedule Bash scripts to run at certain times to automate tasks for you.\u003c/p\u003e\n\u003cp\u003eIf you are using a Linux machine, you can write and execute Bash scripts in terminal. Through the terminal you can use your preferred text editor (nano, vim, etc.) to write your script and save it with the extension .sh. If you are using a MacBook, like me, you can use the terminal or Visual Studio Code to write Bash scripts- just save them with the .bash extension.\u003c/p\u003e","title":"BASH BASICS"},{"content":"Kubernetes provides a framework to orchestrate multiple containers, verses working with one container at a time. To practice managing containers I opted to build a Kubernetes cluster on Raspberry Pis using Rancher. Rancher came about soon after Docker introduced containers to the market, and provides an open-source platform for enterprises to easily use Kubernetes in their environment.\nK3S is a Rancher product that allows you to deploy a Kubernetes cluster using a single binary file thats less than 40MB. K3S is ideal for Raspberry Pis, since traditionally building a Kubernetes cluster on Pis could be too much for them to handle. In addition to K3S this is what I used to build a simple Kubernetes cluster in my home lab:\n3 Raspberry Pi 4s GeekPi Raspberry Pi Cluster Case w/Heat Sinks and Fans 3 power cables and a power supply 3 patch cables and a switch connected to a DNS \u0026amp; router 3 miniSD cards and a CanaKit MicroSD USB Reader A Kubernetes cluster runs by placing containers into groups called pods that run on nodes. A node can be a virtual or physical machine. Each cluster has a master node and at least one worker node that it reports to it. I had 3 Pis to use, so in my cluster I built one master node and two worker nodes. kubectl is the Kubernetes command-line tool that you can use to run commands against a cluster. When building this cluster I used a few kubectl commands to check my progress, but they can be used for a lot more.\nSTEP 1: Build case (optional) I got a case, which is basically a rack, to safely store the 3 Pis for the cluster. The first step was building the case, which consisted of screwing each Raspberry Pi to a base and stacking them one on top of the other. In between each Pi was a fan connected to the Pi below it to provide air flow, and I made sure to place heat sinks on the CPU, RAM and LAN of each Pi. The cooling equipment is probably unnecessary for this small project, but it will be good to have for a more CPU intense project in the future.\nSTEP 2: Set up the Raspberry Pis Next, each Pi will need an SD card with an OS on it. They\u0026rsquo;ll also need to be connected to the internet with a static IP and have SSH abilities. You can go about this any way you want, but I chose to install Raspberry Pi OS 64 bit, a beta version of the new Pi OS, and configure the Pis to be headless.\nI used a laptop running Linux Mint to put the OSs onto the SD cards. This involved downloading the OS and the app image of balenaEtcher onto the laptop and then putting the little SD cards in a USB with a MicroSD card adapter. Once plugged into the laptop, I used balenaEtcher to flash Raspberry Pi OS (64 bit) onto each card.\nOnce done I configured the SD cards so that the Pis would be headless (automatically boot with SSH ability and an IP).\nAfter the SD cards are in the Pis, plug each Pi into a power supply and connected each one to the switch using patch cables.\nSince I configured the Pis to be headless, I checked the leased IPs on the router in the lab to find the IPs of each Pi and set that IP to be static.\nSTEP 3: Change hostnames (optional) Building a Kubernetes cluster on Raspberry Pis involves a lot of SSH-ing, so to make it easier you can rename each Pi. I used the same names I gave each Pi on the router (kmaster, knode1, and knode2). On my PC (a MacBook) I ran sudo nano /etc/host to enter the hosts file. In the file I added three new lines, each containing the IP of a node and its friendly name.\nI then SSH-ed into each Pi one by one and changed its name in the hostname file by running sudo nano /etc/hostname.\nI then entered the /etc/host file like before to remove any instance of the old hostname and replace it with the new one.\nOnce those files are edited, reboot the Pis.\nSTEP 4: Create the master node SSH into the Pi you designated to be the master node.\nFirst, enter the /boot/cmdline.txt file to edit it (sudo nano /boot/cmdline.txt). At the end of the line in the file, add cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory . Save the file and reboot.\nTo install K3S on the master node, run the following command which uses curl to download K3S from its website:\n\u0026lt;code\u0026gt;curl -sfL \u0026lt;a href=\u0026quot;http://get.k3s.io/\u0026quot;\u0026gt;http://get.k3s.io\u0026lt;/a\u0026gt; | sh -\u0026lt;/code\u0026gt; To make sure the K3S installed correctly, and to check its status, use:\nsudo systemctl status k3s\nYou should see that you have the k3s service running on your Pi. To check your nodes, run:\nsudo k3s kubectl get nodes At this point, you should only see your master node up and running.\nTo add agent, or worker, nodes to your master node you need the token the master node generates. To get it:\nsudo cat /var/lib/rancher/k3s/server/node-token\nCopy the token and save it for later.\nSTEP 5: Create agent nodes To create a worker node, SSH into the Raspberry Pi and run:\ncurl -sfL http://get.k3s.io | K3S_URL=https://_your_master_node_IP_:643 K3S_TOKEN=_token_ sh - There are a few variations of commands to set up a worker node using K3S, the one listed above instals K3S and then passes along the IP/Port of the master node and its token so that the workers know who to report to.\nRepeat the above command for each Pi designated to be a worker node.\nSTEP 6: Install K3S on PC It can be inconvenient to SSH into the Kubernetes master node to work on your cluster. You can install K3S onto your PC by first logging on to the master node and copying the configuration file found by running sudo cat /etc/rancher/k3s/k3s.yaml .\nOn your PC, make a directory for the config file ( mkdir ~/.kube) and paste the file into it. Before saving, change the \u0026ldquo;https://localhost:6443\u0026rdquo; line in the file to \u0026ldquo;https://kmaster:6443\u0026rdquo;.\nThen install kubectl on your PC, simple instructions for your PC\u0026rsquo;s OS can be found here.\nFinally, run kubectl get nodes on your PC to see the kubernetes cluster you have created.\n","permalink":"//localhost:1313/raspberry-pi-kubernetes/","summary":"\u003cp\u003eKubernetes provides a framework to orchestrate multiple containers, verses working with one container at a time. To practice managing containers I opted to build a Kubernetes cluster on Raspberry Pis using Rancher. Rancher came about soon after Docker introduced containers to the market, and provides an open-source platform for enterprises to easily use Kubernetes in their environment.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://k3s.io/\"\u003eK3S\u003c/a\u003e is a Rancher product that allows you to deploy a Kubernetes cluster using a single binary file thats less than 40MB. K3S is ideal for Raspberry Pis, since traditionally building a Kubernetes cluster on Pis could be too much for them to handle.  In addition to K3S this is what I used to build a simple Kubernetes cluster in my home lab:\u003c/p\u003e","title":"RASPI KUBERNETES CLUSTER WITH RANCHER"},{"content":"Introduction to Terraform Terraform by Hashicorp is a \u0026ldquo;tool for building, changing, and versioning infrastructure safely and efficiently\u0026rdquo;. Terraform works using infrastructure as code, meaning that build a blueprint of the resources you need in a configuration file (with the .tf extension) and Terraform builds that infrastructure for you. The Terraform package itself is a single binary file, easily downloaded, and is used to build/edit/destroy your resources.\nIn contrast to older versions of infrastructure build tools like Packer, Terraform allows you to create infrastructure across multiple cloud providers and is \u0026ldquo;smart\u0026rdquo; enough to know how many more machines to create in context of what you already have. For example, if you have 5 servers on Azure, but you want 10, Terraform will build 5 more for you to bring you to a total of 10 versus building 10 more servers and leaving you with 15.\nA helpful feature of Terraform is the ability to plan your infrastructure before it is constructed. There are four basic terraform commands: terraform init, terraform plan, terraform apply, and terraform destroy. terraform plan is what allows you to see what resources are being created/destroyed based on your configuration file. If everything checks out, terraform apply builds your infrastructure and reports on what was created. terraform destroy, you guessed it, easily destroys the resources defined in your configuration file and reports on what was destroyed.\nThe ability to create cross-platform resources is due to Terraform\u0026rsquo;s many providers that support API interactions between their platform and Terraform. As you\u0026rsquo;ll see below, once you define the provider of your choice in your configuration file you run terraform init to initialize the directory containing your Terraform config files. If you already have files created running terraform init will push any changes made since the command was run last.\nUsing Terraform to build a VM I used Terraform to create and destroy a VM in the vSphere environment I built in my home computer lab. VMware vSphere is a Terraform provider, and an example configuration file along with information on how to create complex resources can be found on Terraform\u0026rsquo;s website. However, I used a configuration file found on GitHub that worked well for practicing Terraform. These steps assume that you have a vCenter server set up and at least one functioning VM template.\nSTEP 1 Download Terraform\nDepending on the OS of your machine, you can easily download Terraform in a number of ways. Since I use a MacBook and have homebrew installed I ran brew install terraform to download the single binary file. Adding --version to the name of a package will allow you to check 1) that the package actually downloaded and 2) the version you have installed. I quickly ran terraform --version to check that the file installed correctly and noted the version.\nSTEP 2 Create a configuration file\nCreate a file and give it a .tf extension. The first few lines of the configuration file should define the provider you choose to use and list the login information for your account there. In my configuration file I included the username/password of my VMware account and the ip address of my vCenter server. This allows Terraform to be able to \u0026ldquo;log in\u0026rdquo; and create the resources for me. I also included an allow_unverified_ssl line, since my vSphere environment doesn\u0026rsquo;t have a certificate signed by any official authority and I didn\u0026rsquo;t want Terraform to throw a security error. At this point you can run terraform init to check that your configuration file successfully uses Terraform to reach the provider you chose and initialize.\nAfter you define the provider you wish to use, you list the information Terraform requires to work in vCenter. You can write your configuration file in JSON but I used the Hashicorp Configuration Language (HCL), which is easily readable and developed by Hashicorp especially for Terraform. There are a wide variety of options that you can list to create a VM, all are explained on Terraform\u0026rsquo;s website, but the configuration file I used had a few simple data sources:\nvsphere_datacenter: the name of the datacenter where your VMs live vsphere_resource_pool: the name of the resource pool your VMs are under (if you do not have one put your_clustername/Resources in place of the name) vsphere_host: the IP address of the host where you would like your new VM(s) deployed vsphere_datastore: the name of the datastore where you store your VM ISO files (the storage device connected to your VMs) vshpere_network: the name of the network you would like your new VM(s) in vsphere_virtual_machine: the name of the VM template that you would like to use to create your new machine(s) You can see each data source is defined by the \u0026ldquo;data\u0026rdquo; keyword, the official name of the source, and then a friendly name. You then list the name of the appropriate feature in your vSphere environment and a datacenter_id. The datacenter_id is the managed object ID of the your datacenter. The line datacenter_id = data.vsphere_datacenter.dc.id stays the same for each data source since it points to the same place (your datacenter). It is basically a query for the ID of your unique datacenter, and the query looks different for older versions of Terraform.\nOnce you define your data sources, you need to create a resource and call on those data sources to build it. Each resource begins with the \u0026ldquo;resource\u0026rdquo; keyword\u0026quot; and the official name/friendly name. You then give the VM a name, the CPU/memory specifications and a list of managed object IDs. These lines are similar to the datacenter_id variable but instead of calling on your datacenter they call on your: datastore, host, resource pool, VM template, and network. The basic format is:\n\u0026lt;em\u0026gt;source\u0026lt;/em\u0026gt;_id = data.\u0026lt;em\u0026gt;officialname\u0026lt;/em\u0026gt;.\u0026lt;em\u0026gt;friendlyname\u0026lt;/em\u0026gt;.id The only exceptions are the two variables that come from your VM template. Instead of a .id they end with a ._nameofvariable_ since they weren\u0026rsquo;t explicitly listed as data sources. Then list the specifications of the disk of your new VM.\nFinally, you give the new VM you want created from the template a unique configuration. Since I was using an Ubuntu template, I customized using linux_options. I also gave the VM a new IP address/mask/DNS/gateway so that it would be able to connect to the internet as soon as it was created.\n**STEP 3 Terraform plan **\nOnce you have your configuration file written, you can run terraform plan. This allows you to check for errors in the file and fix them, and review the infrastructure you outlined to make sure it what you want. I wrote my config file in Visual Studio Code, and ran terraform commands from the terminal there.\nAfter terraform plan outlines the infrastructure you included in the configuration file it lists how many resources you plan to add/chance/destroy.\n**STEP 4 Terraform apply **\nIf terraform plan doesn\u0026rsquo;t throw any errors and shows the infrastructure you want, run terraform apply to build your resources.\nYou will be asked to confirm that you want the resources built, this point is your last chance to go back and edit your configuration file before the resources are created.\nAs terraform apply was run I was able to watch it execute tasks in my vSphere client. Once it was finished, I had a new VM \u0026ldquo;vm-one\u0026rdquo;!\n**STEP 5 Terraform destroy **\nOnce you are finished with the resources you created, you can run terraform destroy to easily tear down the machines defined in your configuration file.\nAfter I ran terraform destroy I went back to my vSphere Client and saw the vm-one was successfully removed.\nThere are a wide variety of ways you could write the configuration file to build a VM with Terraform, the file I used is only one option. In addition to VMware vSphere, Terraform can be used to create/maintain/destroy resources in Azure, AWS, and more.\n","permalink":"//localhost:1313/terraform-in-vsphere/","summary":"\u003ch4 id=\"introduction-to-terraform\"\u003eIntroduction to Terraform\u003c/h4\u003e\n\u003cp\u003e\u003ca href=\"https://www.terraform.io/intro/index.html\"\u003eTerraform\u003c/a\u003e by Hashicorp is a \u0026ldquo;tool for building, changing, and versioning infrastructure safely and efficiently\u0026rdquo;. Terraform works using infrastructure as code, meaning that build a blueprint of the resources you need in a configuration file (with the \u003ccode\u003e.tf \u003c/code\u003eextension) and Terraform builds that infrastructure for you.  The Terraform package itself is a single binary file, easily downloaded, and is used to build/edit/destroy your resources.\u003c/p\u003e\n\u003cp\u003eIn contrast to older versions of infrastructure build tools like Packer, Terraform allows you to create infrastructure across multiple cloud providers and is \u0026ldquo;smart\u0026rdquo; enough to know how many more machines to create in context of what you already have. For example, if you have 5 servers on Azure, but you want 10, Terraform will build 5 more for you to bring you to a total of 10 versus building 10 more servers and leaving you with 15.\u003c/p\u003e","title":"TERRAFORM IN vSPHERE"},{"content":"Practicing installing/updating/building machines is much more fun when you aren\u0026rsquo;t running the risk of ruining an entire computer. To give myself a safe environment to work in, I have been using VMware\u0026rsquo;s vSphere in our home lab for many of my computer projects. vSphere is a suite of virtualization products that allow you to create and manage VMs. This allows me to work on VMs that can run any OS I like, and if I something goes wrong I can just delete the VM and begin again.\nIn our lab we have two ESXi hosts that share a common storage device- a Synology NAS DS420j (Diskstation). We made the two hosts into a cluster, which means they act as one device and balance the load of the VMs between each other. Below you can see the two ESXi hosts sitting on top of the Diskstation in the lab.\nvSphere provides a variety of components to manage your VMs. Within the lab I created a VM to run vCenter, which allows you manage your server on a single console and create a virtual infrastructure. The vSphere Client is an interface that allows you to connect to the vCenter server from anywhere using the IP address of the VM running vCenter. On this page you can view your clusters, VMs, virtual networks, storage devices and manage all your resources.\nOnce you have a VM created, you can make a template of it to \u0026ldquo;copy and paste\u0026rdquo; an OS and avoid having to do an install for each VM you want. You\u0026rsquo;ll need to wipe out any unique information (IP, MAC address, etc.) in a VM before clicking \u0026ldquo;make a template\u0026rdquo; on vSphere, so that you don\u0026rsquo;t end up with multiple identical machines. Fortunately instructions for making a template of almost any OS can easily be found online.\nYou also have the option to contain VMs in a virtual network that allows the VMs to talk to each other but prevents them from accessing any outside network. A virtual network could be useful for a cybersecurity project or simulating a quarantined network.\nCloud computing and containers are new solutions to avoid having to create and manage VMs yourself, but working with VMware has been a good learning experience. I\u0026rsquo;ve previously used VMware Horizon Client and VMware Fusion in my IT classes but using my VMs as a sandbox for my projects has been a useful way to implement VMware on our home lab.\n","permalink":"//localhost:1313/vmware-and-vsphere/","summary":"\u003cp\u003ePracticing installing/updating/building machines is much more fun when you aren\u0026rsquo;t running the risk of ruining an entire computer. To give myself a safe environment to work in, I have been using VMware\u0026rsquo;s vSphere in our home lab for many of my computer projects. vSphere is a suite of virtualization products that allow you to create and manage VMs. This allows me to work on VMs that can run any OS I like, and if I something goes wrong I can just delete the VM and begin again.\u003c/p\u003e","title":"vSPHERE INTRODUCTION"},{"content":"In addition to text analysis Microsoft\u0026rsquo;s cognitive services allow you to convert speech to text, text to speech, and even translate between spoken languages. This powerful AI service is surprisingly easy to use. I was able to speak into my computers microphone and receive a text translation in one language or a spoken translation in multiple languages \u0026ndash;all with the code provided on Microsoft Learn. You have the option to work with the speech SDK (software development kit) in C# or python, I choose python.\nLike the text analysis cognitive service, you\u0026rsquo;ll need to deploy a resource (for the speech service, deploy the \u0026ldquo;speech\u0026rdquo; resource) in Azure and save its key/region to use in the scripts. Since working with translating speech requires you to use your computer\u0026rsquo;s microphone, the code for this service needs to be executed locally instead of in an online environment like Visual Studio Codespaces. I already had python and Visual Studio Code installed on my laptop, so that is what I used.\nSpeech to text translation First, you need to create a folder to store the project in and then open that folder in your code editor. Once in the folder, create a file named translate_speech.py and copy in the code provided for this exercise on Microsoft Learn. In line 3 insert your Azure speech resource\u0026rsquo;s key and region (To keep my key from being public I removed it for this photo-safety first).\nThis script is commented well, so its easy to walk through. It breaks down speech translation into three key objects:\nSpeechTranslationConfig: accepts the key and region of your Azure resource, sets the source and target language, and creates a name for the speech output TranslationRecognizer: accepts the SpeechTranslationCofig object which calls the method to start the translation TranslationRecognitionResult: returns the result of the translation In this first half of this translate_speech_to_text function you can change the fromLanguage and toLanguage variables to translate to/from many different languages. Below is the second half of the function which consists of an if/elif statement that gives options for output. This way you\u0026rsquo;ll receive output that informs you of what has happened even if the speech couldn\u0026rsquo;t be translated or the translation was cancelled.\nIn the last line you can see the translate_speech_to_text() function being called, which in combination with your Azure resource translates your speech to another language when the script is run. Here you can see my script running which translated \u0026ldquo;Hello World\u0026rdquo; in english to \u0026ldquo;Hallo Welt\u0026rdquo; in dutch.\nSpeech to speech translation-multiple languages The speech cognitive service also allows you to translate speech in one language to speech in multiple other languages using a similar script to the one shown above. In addition to the SpeechTranslationConfig, TranslationRecognizer, and TranslationRecognitionResult objects the script for translating to multiple languages also uses a speech synthesizer object that plays the audio output of the target languages.\nYou can use the same speech Azure resource for this script, I used Visual Studio Code and python here too. First you need to create a new file titled texttomultilang.py or something similar. Then copy in the code provided on Microsoft Learn for this exercise. Like before, you also need to insert your speech resource\u0026rsquo;s key and region before the script\u0026rsquo;s function.\nHere you can see this script is similar to the speech to text script. One difference is the addition of the speech_synthesizer object and multiple target languages in the SpeechTranslationConfig object\u0026rsquo;s dictionary. You can add as many target languages as you would like by adding \u0026ldquo;speech_config.add_target_language(\u0026rsquo;language\u0026rsquo;)\u0026rdquo; to the dictionary.\nAnother difference between this script and the speech to text script is a more complicated if/elif statement at the end of the function. So that pronunciation is correct, its important that the synthesized voice used to \u0026ldquo;speak\u0026rdquo; the translation is the right voice for the language. To make sure that happens an if/else statement for each language the speech is being translated into is nested under the first if in the main if/elif structure. This is the basic setup:\nIs the speech recognized/can it be translated? then print the translations: is the toLanguage language#1? then use language#1's voice is the toLanguage language#2? then use language#2's voice OR Could the speech not be translated? OR Is the speech not recognized? OR Was the translation cancelled? print this error If you add more toLanguages in the SpeechTranslationConfig\u0026rsquo;s dictionary, make sure you add the language(s) to the nested if/else statement and name the right voice.\nWhen the script is run your speech is printed, followed by the translations into the languages of your choice. In addition to a printed translation, you receive a spoken translation with an appropriate accent.\nBeing able to translate between languages in near real-time could be used in meetings, conferences, or presentations that have participants from all over the world. Regardless of how its used, working with the speech cognitive service is great practice in using python ( or C#) to work with AI.\n","permalink":"//localhost:1313/cognitive-services-translation/","summary":"\u003cp\u003eIn addition to text analysis Microsoft\u0026rsquo;s cognitive services allow you to convert speech to text, text to speech, and even translate between spoken languages. This powerful AI service is surprisingly easy to use. I was able to speak into my computers microphone and receive a text translation in one language or a spoken translation in multiple languages \u0026ndash;all with the \u003ca href=\"https://docs.microsoft.com/en-us/learn/modules/translate-speech-speech-service/\"\u003ecode \u003c/a\u003eprovided on Microsoft Learn. You have the option to work with the speech SDK (software development kit) in C# or python, I choose python.\u003c/p\u003e","title":"COGNITIVE SERVICES-TRANSLATING SPEECH"},{"content":"As I continue to study artificial intelligence I\u0026rsquo;ve been able to practice using Microsoft\u0026rsquo;s cognitive services. I was first introduced to cognitive services during the virtual Microsoft Build 2020 conference, and I\u0026rsquo;ve enjoyed getting to use them for myself. Since creating and training your own artificial intelligence/machine learning software is all kinds of complicated, cognitive services allow anyone to use AI in their projects without having to write much code. All you have to do is call the service that you want, and it will do the hard work for you.\nThe first step in using cognitive services is to deploy the appropriate resource in Azure. In the case of text analytics you\u0026rsquo;ll deploy the text analytics resource, but if you are wanting to use speech cognitive services you\u0026rsquo;d deploy a speech resource, etc. When deploying resources in Azure the basic steps are:\nSign into you Azure account Select \u0026ldquo;create a new resource\u0026rdquo; Search for the resource you want and click \u0026ldquo;create\u0026rdquo; Give the resource a unique name, assign it to a subscription, a pricing tier, and a resource group. Here you can create a new resource group to place the resource in if you would like. Cognitive services require that you have a key (found under the \u0026ldquo;keys and endpoints\u0026rdquo; menu of a resource) and that you know which location your resource is deployed in. This allows you to use your resource to talk to Microsoft\u0026rsquo;s cognitive service through API (application programming interface) calls or SKDs (software development kits).\nThe text analytics service allows you to detect the language of written text, key phrases, sentiment, and key elements that state intent called \u0026ldquo;entities\u0026rdquo;.\nI used text analytics to detect sentiment in text, although the process is similar for any action using text analytics. The text analytics landing page \u0026ldquo;https://[location].dev.cognitive.microsoft.com/docs/services/TextAnalytics.V2.0\u0026rdquo; (where [location] is the region you deployed your text analytics resource) allows you to easily use POST methods to call the Text Analytics API. I selected the \u0026ldquo;sentiment\u0026rdquo; tab where you choose your region and enter the key of your text analytics resource. This information is included in the header section of the request so that it can be made from the right place.\nThe request body itself consists of a document array. Each document has a language, id and text property that defines the text the text analytics service will be analyzing. You can pass documents up to 5,000 characters, and a single request can have 1,000 documents.\nOnce you enter the documents you want and give them the appropriate language property, a unique id and some text you can see the complete http request that will be sent to the Text Analytics API.\nOnce you send the request you\u0026rsquo;re given a response that lists the id of each document followed by their score. When detecting sentiment the Text Analytics API scores the text from 0-1. 0 being very negative sentiment and 1 being very positive. Below you can see how my three documents were ranked, the first being very positive, the second being neutral, and the third being very negative.\nThis particular service would be useful analyzing customer feedback or comments on a project. It\u0026rsquo;s a very powerful way to examine text without having to create your own AI or write any code (beyond creating your documents). All of the methods offered by the Text Analytics API could be helpful time-savers in a business and serve as a great intro to working with Microsoft\u0026rsquo;s cognitive services.\n","permalink":"//localhost:1313/cognitive-services-text/","summary":"\u003cp\u003eAs I continue to study artificial intelligence I\u0026rsquo;ve been able to practice using Microsoft\u0026rsquo;s cognitive services. I was first introduced to cognitive services during the virtual Microsoft Build 2020 conference, and I\u0026rsquo;ve enjoyed getting to use them for myself. Since creating and training your own artificial intelligence/machine learning software is all kinds of complicated, cognitive services allow anyone to use AI in their projects without having to write much code.  All you have to do is call the service that you want, and it will do the hard work for you.\u003c/p\u003e","title":"COGNITIVE SERVICES-SENTIMENT IN TEXT"},{"content":"Arch Linux\u0026rsquo;s official website describes Arch as a distribution that \u0026ldquo;tries to Keep It Simple\u0026rdquo;. It also states that Arch is not for those without the ability or time for a \u0026ldquo;do-it-yourself\u0026rdquo; system\u0026ndash;which is true. I recently built a machine that needed a lightweight OS, so Arch Linux was a good choice. Installing Arch Linux can seem complicated, but understanding what needs to happen in each step of the install makes it feel fun and easy. Here\u0026rsquo;s what I did:\nThe Arch Linux installation guide lists each step to install Arch and provides links to a variety of wiki pages that are worth reviewing before you begin. I opted to practice installing Arch Linux on a virtual machine in my home lab, so I downloaded an Arch iso and spun up a VM.\nBIOS and UEFI are both interfaces that boot the operating system, but UEFI is newer and more powerful. My VM allowed BIOS, so that is what I used, but it is important to remember which you chose for steps later in the install.\nWhether you are installing Arch Linux on a VM or real computer, the first step is to confirm you can connect to the internet on the machine you are running the install. With Arch you are building the OS from basically nothing, and without an internet connection you wont be able to install the packages required for a functioning computer. Connect to an ethernet cable/wireless LAN and check your network with a few commands:\nip link ip addr ping google.com If your link is up, you have an IP address, and you can ping a website you are ready to continue with the install.\nUpdating the system clock is a surprisingly important step in installing Arch Linux. I passed over this step at first and it prevented me from reaching any mirror servers (mentioned later) to install essential packages. To avoid that you can use timedatectl commands to update the clock:\ntimedatectl status shows you the timezone/time set on your machine\ntimedatectl list-timezones to view all timezones, and find the one for your area.\ntimedatectl set-timezone _timezone _to set the right timezone\nA partition is a logical division of storage that allows you to allot a set amount of space on a computer\u0026rsquo;s disk for different purposes. Each partition stores 1 file system which organizes the partition\u0026rsquo;s information. Partitions can be mounted on a mount point, which gives the partition access to the file system through a directory. It is possible to make dynamic partitions that grow/shrink depending on disk usage with LVM (logical volume manger), but I opted to use regular partitions for this basic install.\nThere are two types of partitions: primary and extended. A primary partition is your typical partition that stores a complete file system. However, only 4 primary partitions are allowed on a single disk, so if you would like more 5+ partitions you have to use an extended partition. You are only allowed 1 extended partition per disk, and it is counted as 1 of your 4 primary partitions, but extended partitions can contain an unlimited number of smaller logical partitions.\nSome partitions commonly used are:\nThe root partition: contains many top-level directories like /etc and/bin that store important information \u0026amp; configuration files for a system. Where the primary filesystem is mounted and where all other file systems stem. The swap partition: provides memory that can be used as \u0026ldquo;overflow\u0026rdquo; virtual RAM The boot partition: stores the kernel, images, and the bootloader, not required for a system to function but used during the boot process upgrades. At this point it is important to remember if you chose UEFI or BIOS to boot your machine. If your machine is booting in UEFI mode, you will use a globally unique identifiers (GUID) partition table to create your partitions. GUID is alternative partitioning style that aims to fix quirks found in older partitioning methods. For GUID you\u0026rsquo;ll create a root partition, swap partition, and a EFI system (boot) partition. BIOS however, uses a master boot record (MBR) to store the OS bootloader and the partition table. So if you are using BIOS, as I did, you\u0026rsquo;ll only need a root partition and a swap partition.\nI used fdisk to manipulate the disk\u0026rsquo;s partition table.\nI viewed the partition table with fdisk -l then began editing the main disk by entering fdisk /dev/sda since my VM\u0026rsquo;s disk was /dev/sda.\nfdisk\u0026rsquo;s command mode allows you to enter single letters to specify actions. Entering \u0026ldquo;m\u0026rdquo; will show you all possible commands and make navigating fdisk easy.\nType \u0026ldquo;n\u0026rdquo; to create a new partition.\nType \u0026ldquo;p\u0026rdquo; to create a primary partition, or \u0026ldquo;l\u0026rdquo; to create a logical partition (in an extended partition) .\nYou\u0026rsquo;ll be ask to enter the number of the partition, which will give the partition the name /dev/sda#. I entered 1 for my first partition, 2 for my second, etc.\nNext specify the sector of the disk where you want the partition to start. Press Enter to accept the first free sector.\nLastly, specify the last sector of the disk you want the partition to occupy. This is how you control the size of your partition. You can use the format +_size_K/M/G to set the size of the partition in kilobytes (K), megabytes (M), or gigabytes (G). If you want to use all remaining space, press Enter.\nSwap partitions should be 512+ MiB Boot partitions (if using) should be 260-512 MiB Root partitions should take up the remainder of the device. Typing \u0026ldquo;w\u0026rdquo; will write the changes and put you out of fdisk\u0026rsquo;s command mode.\nEnter fdisk -l to view your partition table again. You should see the new partitions you created and be able to check that their names/sizes are correct.\nFinally, your root partition needs to be bootable.\nEntering fdisk -u /dev/_partition_ puts you into the command mode of a particular partition. Enter the command mode of your root partition and enter \u0026ldquo;a\u0026rdquo; to toggle a bootable flag.\nThen confirm the number of the partition you want to make bootable (In my case, 1) and enter \u0026ldquo;w\u0026rdquo; to write the changes. If you view your partition table again, your root partition will have a \u0026ldquo;*\u0026rdquo; next to it, signifying it is bootable.\nOnce the partitions are created, they must be given a file system. I gave my root partition an ext4 file system and intialized my swap partition using:\nmkfs.ext4 /dev/\u0026lt;em\u0026gt;root_partition\u0026lt;/em\u0026gt; mkswap /dev/\u0026lt;em\u0026gt;swap_partition\u0026lt;/em\u0026gt; swapon /dev/\u0026lt;em\u0026gt;swap_partition\u0026lt;/em\u0026gt; The root partition\u0026rsquo;s file system needs to be mounted through the file system\u0026rsquo;s /mnt directory.\nmount /dev/\u0026lt;em\u0026gt;root_partition\u0026lt;/em\u0026gt; /mnt After the partitions are made, you need to install a basic package, the Linux kernel (or kernel of choice), and firmware. This is done using pacstrap, which contacts servers across the globe called mirror servers to get essential packages for the system to run and download them.Like apt or get, pacman is the package installer for Arch Linux.\nAll mirrors are stored in /etc/pacman.d/mirrorlist, the higher the mirror on the list the more priority it has when downloading a package. You can edit the /etc/pacman.d/mirrorlist file using nano or vi to delete all mirror servers not in your area to speed up the process. Here, I used nano.\nnano /etc/pacman.d/mirrorlist ctrl+K to delete all mirror servers not in my region. Then install the packages. The \u0026ldquo;linux-firmware option can be left off if your installing arch on a VM or a container.\npacstrap /mnt base linux linux-firmware After the essential packages/kernel is installed, you can install any other packages you\u0026rsquo;d like. I downloaded a nano package, because you need a text editor to complete the last few steps of installing Arch Linux.\npacman -S nano An fstab file allows you to set which partitions will be automatically mounted when the system boots. To generate the fstab file run:\ngenfstab -U /mnt \u0026gt;\u0026gt; /mnt/etc/fstab Since the kernel and packages have been installed, you can now change root into the new Arch Linux system.\narch-chroot /mnt Set the timezone, I used the same region as when I updated the system clock in step 2.\nln -sf /usr/share/zoneinfo/\u0026lt;em\u0026gt;Region\u0026lt;/em\u0026gt;/\u0026lt;em\u0026gt;City\u0026lt;/em\u0026gt; /etc/localtime Generate a /etc/adjtime file with hwclock:\nhwclock --systohc Lastly, there are a few config files that need to be set.\nGenerate locales, which are used for correctly displaying information that depends on where you are in the world (monetary symbols, date/time, alphabets, etc).\nlocale-gen Then edit /etc/locale.conf with the text editor package installed in step 4, set the LANG variable to the correct language.\nThe hostname file sets the unique name given to your machine. Create the /etc/hostname file with your text editor, and enter the name you want to give your Arch Linux machine (I named mine Arch_Linux).\nnano /etc/hostname \u0026lt;em\u0026gt;hostname \u0026lt;/em\u0026gt; You then add your hostname to the /hosts file, which is a static table lookup for hostnames. Here you\u0026rsquo;ll add your host\u0026rsquo;s loopback address.\nThen, set the root password.\npasswd Finally, install a bootloader so you can remove the installation medium (USB, CD, .iso image, etc.) and the OS will be able to boot on the machine. This VM is a non-UEFI system, and while I chose to use grub there are many bootloaders to choose from. The commands to install grub on a non-UEFI system are:\npacman -S grub grub-install /dev/sda grub-mkconfig -o /boot/grub/grub.cfg When you reboot your machine, you are then able to sign in as root and begin working on Arch Linux!\n","permalink":"//localhost:1313/arch-linux-install/","summary":"\u003cp\u003eArch Linux\u0026rsquo;s official website describes Arch as a distribution that \u0026ldquo;tries to Keep It Simple\u0026rdquo;. It also states that Arch is not for those without the ability or time for a \u0026ldquo;do-it-yourself\u0026rdquo; system\u0026ndash;which is true. I recently built a machine that needed a lightweight OS, so Arch Linux was a good choice. Installing Arch Linux can seem complicated, but understanding what needs to happen in each step of the install makes it feel fun and easy. Here\u0026rsquo;s what I did:\u003c/p\u003e","title":"ARCH LINUX INSTALL"},{"content":"For the home-lab, we used the following devices:\nUbiquiti EdgeRouter X Ubiquiti UniFi 24 port switch 2 VMware ESXi Hosts A Synology NAS DS420j (Diskstation) Building the home lab also required quite a few patch cables to connect devices, and I made most of them myself. This required UTP cable, plastic clips for the ends, a cable crimping tool, and a lot of patience. The arrangement of the wires, or the pinout, had to be in a specific order.\nThe first step in building the home-lab was finding a way to connect our home network to the garage. The lab needed to be in it\u0026rsquo;s own subnet so that anything done in the lab (or anything that goes wrong in the lab) doesn\u0026rsquo;t bleed over into the home network. To extend the home network to the garage, we used two Ubiquiti UniFi access points. One was connected to the home network and then placed on top of the house. The other was put on the garage, and the two access points were aligned.\nThe AP on the garage was plugged into port 4 on the Ubiquiti EdgeRouter X, where a new 192.168.100.x/24 network was set. The router connected to the switch, which in turn connected to the two ESXi hosts and the Diskstation storage device.\nThe Edge Router X doubles as a DHCP server, and once it gave the ESXi hosts and the Diskstation their own IP address the home-lab was ready for new projects.\n","permalink":"//localhost:1313/home-lab-setup/","summary":"\u003cp\u003eFor the home-lab, we used the following devices:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUbiquiti EdgeRouter X\u003c/li\u003e\n\u003cli\u003eUbiquiti UniFi 24 port switch\u003c/li\u003e\n\u003cli\u003e2 VMware ESXi Hosts\u003c/li\u003e\n\u003cli\u003eA Synology NAS DS420j (Diskstation)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBuilding the home lab also required quite a few patch cables to connect devices, and I made most of them myself. This required UTP cable, plastic clips for the ends, a cable crimping tool, and a lot of patience. The arrangement of the wires, or the pinout, had to be in a specific order.\u003c/p\u003e","title":"HOME-LAB SETUP"},{"content":"The final step in building my bird cam was creating a way to view the bird photos on demand. To do this I utilized Docker on a third Raspberry Pi by installing Docker and Docker Compose.\nsudo apt install docker sudo apt install docker-compose Compose is a useful Docker tool that, among other things, allows you to run multi-container applications configured in a YAML file. On Docker Hub a useful photo gallery image linuxserver/photoshow has already been created, so I used that image to serve up the bird photos. At this point, I had already configured an NFS server and made this Pi an NFS client via a shared folder. To create the photo gallery, I first signed into my personal Docker account. I then added two subdirectories to my home directory on the Pi: a config directory, and a thumbs directory.\nsudo docker login sudo mkdir config sudo mkdir thumbs I left these directories empty, since in this case they only need to exist for the YAML file used to configure Docker Compose. I then created the YAML file named docker-compose.yml and included the following content:\nversion: \u0026quot;2.1\u0026quot; services: photoshow: image: linuxserver/photoshow container_name: photoshow environment: - PUID=1000 - PGID=1000 - TZ=America/Louisville volumes: - /config:/config - /\u0026lt;em\u0026gt;photos-path-on-server\u0026lt;/em\u0026gt;:\u0026lt;em\u0026gt;/photos-path-on-client\u0026lt;/em\u0026gt; - /thumbs:/thumbs ports: - 80:80 restart: unless-stopped I configured a number of things in the YAML file :\nImage: Since I wanted to use photoshow to host my bird photos, I included here the exact name of the image as it appears on Docker Hub TZ: I put the appropriate timezone to match my location Volumes: Here I set three volumes. On each, the path on the left of the colon is the path on the server, the path on the right is the path within the container. In this case, the config and thumbs directories are the same for both (they\u0026rsquo;re empty). To map the path to the photos however, on the left of the colon I put the location of the shared folder on the NFS server. On the left is the location of the shared folder on this Raspberry Pi. Finally, I started Docker Compose. The -d option allows Compose to run the photoshow image in the background, so that the command line is free.\nsudo docker-compose up -d To view the photo gallery, all I have to do is enter the IP of the Pi running the image followed by \u0026ldquo;:80\u0026rdquo; since the YAML file configured port 80 to run the website. There, after making an account, I can view new bird photos everyday.\nFor this project, one Raspberry Pi Zero W was outfitted with a camera scheduled to take photos all day long. The Zero mapped those photos to a shared folder with an NFS server, which in turn shared those photos with another Pi running Docker\u0026rsquo;s photoshow image.\n","permalink":"//localhost:1313/birdcam-pt3/","summary":"\u003cp\u003eThe final step in building my bird cam was creating a way to view the bird photos on demand. To do this I utilized Docker on a third Raspberry Pi by installing Docker and Docker Compose.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo apt install docker \nsudo apt install docker-compose \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCompose is a useful Docker tool that, among other things, allows you to run multi-container applications configured in a YAML file.  On Docker Hub a useful photo gallery image linuxserver/photoshow has already been created, so I used that image to serve up the bird photos.  At this point, I had already configured an NFS server and made this Pi an NFS client via a shared folder. To create the photo gallery, I first signed into my personal Docker account. I then added two subdirectories to my home directory on the Pi: a config directory, and a thumbs directory.\u003c/p\u003e","title":"BIRDCAM Pt. 3"},{"content":"After setting up the NFS server, but before making any NFS clients, I configured another Raspberry Pi to take the photos for my Bird Cam project. For this I used a Raspberry Pi Zero W and the appropriate camera attachment. Instead of using a keyboard/monitor to configure the Zero, I created a headless setup to allow the Zero to connect to wifi automatically when plugged in.\nFirst, I put the Raspbian Lite OS on a mini SD card. Once the OS was finished downloading, I removed the card from my computer and inserted it again. On my computer\u0026rsquo;s terminal I entered the SD card\u0026rsquo;s boot directory (found with df -h) and created a new file to configure the wifi information for my home network.\ncd \u0026lt;em\u0026gt;card's-boot-directory\u0026lt;/em\u0026gt; vim wpa_supplicant.conf In the file I added:\ncountry=US ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=\u0026quot;\u0026lt;em\u0026gt;Network's-name\u0026lt;/em\u0026gt;\u0026quot; psk=\u0026quot;\u0026lt;em\u0026gt;Network's-password\u0026lt;/em\u0026gt;\u0026quot; key_mgmt=WPA-PSK } This allowed the Raspberry Pi Zero to automatically know my home network\u0026rsquo;s wifi info and be able to connect without having to attach a keyboard and enter the information manually.\nThe final step in creating a headless setup on the Zero is to enable ssh so that you are able to connect to the Pi through secure shell. To do this, I created an empty file in the boot directory of the SD card named ssh.\ncd \u0026lt;em\u0026gt;card's-boot-directory \u0026lt;/em\u0026gt; touch ssh Then, after plugging in the Raspberry Pi Zero and finding it\u0026rsquo;s IP on my network, I was able to ssh into it and set up the camera.\nTo take photos with the Zero I used a Raspberry Pi camera and an adapter for the Zero\u0026rsquo;s tiny camera port. Once the adapter was snapped into the camera on one end and the Zero on the other (with the circuitry facing the back of the Zero) I configured the camera and tested it. To do so I entered the configuration program for the Raspberry Pi:\nsudo raspi-config I then enabled the camera under \u0026ldquo;Interfacing Options\u0026rdquo;. To test that the camera works, I ran the **raspistill **command:\nraspistill -v -o \u0026lt;em\u0026gt;name\u0026lt;/em\u0026gt;.jpg Once the camera was attached and working, I created a directory to store the photos using **mkdir **(this should be the same directory used to mount the NFS share, so that the photos are mapped to the NFS server).\nI wanted to configure the camera to take photos every half hour from 8am-5pm. I did this by writing a simple bash script to use raspistill to take photos and give them a unique name of the date/time.\nvim \u0026lt;em\u0026gt;script\u0026lt;/em\u0026gt;.sh #!/bin/bash DATE=$(date +\u0026quot;%Y-%m-%d_%H%M\u0026quot;) raspistill -o /\u0026lt;em\u0026gt;path-to-photos\u0026lt;/em\u0026gt;/$DATE.jpg I then made that script executable:\nchmod +x \u0026lt;em\u0026gt;script\u0026lt;/em\u0026gt;.sh Finally, I created two cron jobs, one to take a photo every hour and another to take a photo every half hour. cronab -e will place you in the cron editor.\ncrontab -e 0 8,9,10,11,12,13,14,15,16,17 * * * ./\u0026lt;em\u0026gt;script\u0026lt;/em\u0026gt;.sh 30 8,9,10,11,12,13,14,15,16,17 * * * ./\u0026lt;em\u0026gt;script\u0026lt;/em\u0026gt;.sh Once I added the jobs I wanted to schedule I entered Ctrl+X, Y, Enter to exit. To double check the jobs, I viewed them with crontab -l. Now, the Raspberry Pi Zero W is able to take multiple photos a day, and store them on the NFS server through the shared folder.\n","permalink":"//localhost:1313/birdcam-pt2/","summary":"\u003cp\u003eAfter setting up the NFS server, but before making any NFS clients, I configured another Raspberry Pi to take the photos for my Bird Cam project. For this I used a Raspberry Pi Zero W and the appropriate camera attachment. Instead of using a keyboard/monitor to configure the Zero, I created a headless setup to allow the Zero to connect to wifi automatically when plugged in.\u003c/p\u003e\n\u003cp\u003eFirst, I put the Raspbian Lite OS on a mini SD card. Once the OS was finished downloading, I removed the card from my computer and inserted it again. On my computer\u0026rsquo;s terminal I entered the SD card\u0026rsquo;s boot directory (found with \u003cstrong\u003edf -h\u003c/strong\u003e) and created a new file to configure the wifi information for my home network.\u003c/p\u003e","title":"BIRDCAM Pt. 2"},{"content":"In this project I wanted to set up a camera to take pictures of pretty birds at a feeder, and then view those photos online. To do so I used three Raspberry Pis, NFS, and Docker.\nOne Raspberry pi served as the NFS server, while the other two were NFS clients. One of the clients was a Raspberry Pi zero w outfitted with a camera to take the pictures and store them on the NFS server. The other client hosted the docker container linuxserver/photoshow that accessed the NFS server to display the photos in a gallery.\nSince I wanted to take multiple photos a day, dedicating one Pi to storing the photos freed up the other Pis to do their respective jobs. This was done by making one Pi an NFS (Network File System) server. NFS allows client computers to access files over a network as if those files were stored locally. First, I configured one Pi as the NFS server. To do so, I first made sure all packages were updated on the Raspbian OS.\nsudo apt-get update sudo apt-get upgrade Then, I installed the nfs-kernel-server package that builds the protocol to handle the server side of NFS.\nsudo apt-get install nfs-kernel-server -y Next I gave the pi user/group ownership of the folder I want shared (the folder for the bird photos). I also used the find command to change the permissions of the directories and files in that folder.\nsudo chown -R pi:pi \u0026lt;em\u0026gt;/path-to-folder\u0026lt;/em\u0026gt; sudo find \u0026lt;em\u0026gt;/path-to-folder\u0026lt;/em\u0026gt; -type d -exec chmod 755 {} \\; sudo find \u0026lt;em\u0026gt;/path-to-folder\u0026lt;/em\u0026gt; -type f -exec chmod 644 {} \\; To allow the NFS protocol to know what directories to share, we need to include that path in the /etc/exports file. In addition to the path, I included a variety of options:\n/\u0026lt;em\u0026gt;path-to-folder\u0026lt;/em\u0026gt; *(rw,all_squash,insecure,async,no_subtree_check,anonuid=1000,anongid=1000) *: allow all IPs to access this share, you can also include specific IPs rw: allow reading/writing all_squash: maps uids and gids to an anonymous user insecure: allows clients that do not use a reserved NFS port async: allows the NFS server to improve performance if the server crashes, even if that causes data to be corrupted no_subtree_check: disables subtree checking, improving reliability of NFS anonuid: this is the UID of the pi user anongid: this is the GID of the pi user Once the Pi with the NFS server was set up, I configured the other Raspberry Pis to be clients. On each Pi, I first installed NFS tools, and created a virtual folder to serve as a mount-point to the NFS share. The location of the folder doesn\u0026rsquo;t matter-as long as you remember the path to it.\nsudo apt-get install nfs-common -y sudo mkdir -p \u0026lt;em\u0026gt;/path-to-folder\u0026lt;/em\u0026gt; Like on the NFS server, I also changed the permissions of the mount point to the pi user and group (or the user\u0026rsquo;s user/group name).\nsudo chown -R pi:pi \u0026lt;em\u0026gt;/path-to-folder\u0026lt;/em\u0026gt; Next, mount the NFS share to the mount-point. First, name the IP of the NFS server and then the path to the shared folder on the NFS server. Then name the path to the shared folder on the client.\nsudo mount \u0026lt;em\u0026gt;ip-of-NFS-server:path-on-server path-on-client\u0026lt;/em\u0026gt; Edit the /etc/fstab file to include the mount and permissions so that the NFS share will automatically mount when your machine boots. One done editing the fstab file, enter Ctrl+x, Y, Enter to save and exit.\nsudo nano /etc/fstab \u0026lt;em\u0026gt;ip-of-NFS-server:path-on-server path-on-client\u0026lt;/em\u0026gt; nfs rw 0 0 Now, one Raspberry Pi is an NFS server and the other two are connected to it via a shared folder. This allows the Pi with the camera to store the photos it takes on the share, and the Pi running the photo gallery container to access those photos, all without taking up any storage on the other two Pis.\n","permalink":"//localhost:1313/birdcam-pt1/","summary":"\u003cp\u003eIn this project I wanted to set up a camera to take pictures of pretty birds at a feeder, and then view those photos online.  To do so I used three Raspberry Pis, NFS, and Docker.\u003c/p\u003e\n\u003cp\u003eOne Raspberry pi served as the NFS server, while the other two were NFS clients. One of the clients was a Raspberry Pi zero w outfitted with a camera to take the pictures and store them on the NFS server. The other client hosted the docker container linuxserver/photoshow that accessed the NFS server to display the photos in a gallery.\u003c/p\u003e","title":"BIRDCAM Pt. 1"},{"content":"About Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean nec dolor in magna lobortis egestas. Suspendisse eu erat tempor, tristique neque eu, convallis nulla. Curabitur vel bibendum lacus, at semper mauris. Suspendisse aliquet commodo ex, sed sagittis metus aliquam id. Maecenas feugiat rutrum lorem vel imperdiet. Nullam ornare lectus ut enim finibus, et porttitor mi tincidunt. Aenean lacinia, leo quis vehicula eleifend, quam libero sagittis erat, at euismod augue mauris et sapien. Sed id vehicula lectus, sit amet auctor ipsum. Nunc sed massa vel ex condimentum aliquam in a enim. Cras enim sem, tristique eu enim ac, congue commodo mi. Integer quis orci at dolor blandit eleifend.\nSchool Sed quis pulvinar nisi, sed dapibus lacus. Mauris tempus ex ut ipsum facilisis interdum. Integer feugiat urna sed feugiat tempor. Quisque dictum vestibulum feugiat. Nunc nec est volutpat lorem imperdiet egestas sit amet ut orci. Aliquam venenatis neque ipsum, a placerat lectus cursus at. Donec pellentesque tellus hendrerit faucibus posuere. Etiam gravida consequat lectus, volutpat consectetur velit convallis nec. Praesent sagittis luctus dignissim.\n","permalink":"//localhost:1313/page/about/","summary":"\u003ch2 id=\"about\"\u003eAbout\u003c/h2\u003e\n\u003cp\u003eLorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean nec dolor in magna lobortis egestas. Suspendisse eu erat tempor, tristique neque eu, convallis nulla. Curabitur vel bibendum lacus, at semper mauris. Suspendisse aliquet commodo ex, sed sagittis metus aliquam id. Maecenas feugiat rutrum lorem vel imperdiet. Nullam ornare lectus ut enim finibus, et porttitor mi tincidunt. Aenean lacinia, leo quis vehicula eleifend, quam libero sagittis erat, at euismod augue mauris et sapien. Sed id vehicula lectus, sit amet auctor ipsum. Nunc sed massa vel ex condimentum aliquam in a enim. Cras enim sem, tristique eu enim ac, congue commodo mi. Integer quis orci at dolor blandit eleifend.\u003c/p\u003e","title":"About"},{"content":"Contact Aenean ipsum justo, semper eu nisl ut, pretium tincidunt sem. Praesent et diam sit amet lacus lobortis dictum a id lacus. Quisque hendrerit sit amet turpis eu varius. Ut id lorem ac felis ultrices tincidunt. Pellentesque consequat arcu ac fringilla imperdiet. Phasellus pellentesque, sapien non pulvinar blandit, sapien ante aliquet felis, vel porttitor sapien ante in lacus. Fusce non urna aliquet, malesuada nibh vel, luctus urna. Phasellus ut lacus molestie, varius purus quis, malesuada lorem.\n","permalink":"//localhost:1313/page/contact/","summary":"\u003ch2 id=\"contact\"\u003eContact\u003c/h2\u003e\n\u003cp\u003eAenean ipsum justo, semper eu nisl ut, pretium tincidunt sem. Praesent et diam sit amet lacus lobortis dictum a id lacus. Quisque hendrerit sit amet turpis eu varius. Ut id lorem ac felis ultrices tincidunt. Pellentesque consequat arcu ac fringilla imperdiet. Phasellus pellentesque, sapien non pulvinar blandit, sapien ante aliquet felis, vel porttitor sapien ante in lacus. Fusce non urna aliquet, malesuada nibh vel, luctus urna. Phasellus ut lacus molestie, varius purus quis, malesuada lorem.\u003c/p\u003e","title":"Contact"}]