<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>NEURAL NETWORKS &amp; TENSORFLOW - Capstone Pt.2 | Olivia Snowden&#39;s Blog</title>
<meta name="keywords" content="code, AI">
<meta name="description" content="In my first Capstone post I gave a rundown of basic AI terms and how to use Tensorflow to create your own machine learning (ML) script.
I&rsquo;m using Tensorflow to write a script that can process images of roads and determine whether there is an obstacle in the road or not. Since I&rsquo;m studying self-driving cars, I&rsquo;d like to see if using biased data to train an object detection script affects the model&rsquo;s performance. To do this, I&rsquo;m using a training dataset of clear, bright road images to train a neural network and then testing that script with corrupted images of roads. If the model can&rsquo;t recognize obstacles in images that are corrupted, then the biased training dataset did have an affect on the model.">
<meta name="author" content="Olivia Snowden">
<link rel="canonical" href="//localhost:1313/capstone-pt2/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.7f8702ad4c3159231ff7aa5f7bf1a574a69e60a988350dee1489d2c17b25d91e.css" integrity="sha256-f4cCrUwxWSMf96pfe/GldKaeYKmINQ3uFInSwXsl2R4=" rel="preload stylesheet" as="style">
<link rel="icon" href="//localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="//localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="//localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="//localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="//localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="//localhost:1313/capstone-pt2/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="//localhost:1313/capstone-pt2/">
  <meta property="og:site_name" content="Olivia Snowden&#39;s Blog">
  <meta property="og:title" content="NEURAL NETWORKS & TENSORFLOW - Capstone Pt.2">
  <meta property="og:description" content="In my first Capstone post I gave a rundown of basic AI terms and how to use Tensorflow to create your own machine learning (ML) script.
I’m using Tensorflow to write a script that can process images of roads and determine whether there is an obstacle in the road or not. Since I’m studying self-driving cars, I’d like to see if using biased data to train an object detection script affects the model’s performance. To do this, I’m using a training dataset of clear, bright road images to train a neural network and then testing that script with corrupted images of roads. If the model can’t recognize obstacles in images that are corrupted, then the biased training dataset did have an affect on the model.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2020-11-11T00:00:00+00:00">
    <meta property="article:modified_time" content="2020-11-11T00:00:00+00:00">
    <meta property="article:tag" content="Code">
    <meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NEURAL NETWORKS &amp; TENSORFLOW - Capstone Pt.2">
<meta name="twitter:description" content="In my first Capstone post I gave a rundown of basic AI terms and how to use Tensorflow to create your own machine learning (ML) script.
I&rsquo;m using Tensorflow to write a script that can process images of roads and determine whether there is an obstacle in the road or not. Since I&rsquo;m studying self-driving cars, I&rsquo;d like to see if using biased data to train an object detection script affects the model&rsquo;s performance. To do this, I&rsquo;m using a training dataset of clear, bright road images to train a neural network and then testing that script with corrupted images of roads. If the model can&rsquo;t recognize obstacles in images that are corrupted, then the biased training dataset did have an affect on the model.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "//localhost:1313/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "NEURAL NETWORKS \u0026 TENSORFLOW - Capstone Pt.2",
      "item": "//localhost:1313/capstone-pt2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "NEURAL NETWORKS \u0026 TENSORFLOW - Capstone Pt.2",
  "name": "NEURAL NETWORKS \u0026 TENSORFLOW - Capstone Pt.2",
  "description": "In my first Capstone post I gave a rundown of basic AI terms and how to use Tensorflow to create your own machine learning (ML) script.\nI\u0026rsquo;m using Tensorflow to write a script that can process images of roads and determine whether there is an obstacle in the road or not. Since I\u0026rsquo;m studying self-driving cars, I\u0026rsquo;d like to see if using biased data to train an object detection script affects the model\u0026rsquo;s performance. To do this, I\u0026rsquo;m using a training dataset of clear, bright road images to train a neural network and then testing that script with corrupted images of roads. If the model can\u0026rsquo;t recognize obstacles in images that are corrupted, then the biased training dataset did have an affect on the model.\n",
  "keywords": [
    "code", "AI"
  ],
  "articleBody": "In my first Capstone post I gave a rundown of basic AI terms and how to use Tensorflow to create your own machine learning (ML) script.\nI’m using Tensorflow to write a script that can process images of roads and determine whether there is an obstacle in the road or not. Since I’m studying self-driving cars, I’d like to see if using biased data to train an object detection script affects the model’s performance. To do this, I’m using a training dataset of clear, bright road images to train a neural network and then testing that script with corrupted images of roads. If the model can’t recognize obstacles in images that are corrupted, then the biased training dataset did have an affect on the model.\nIts safe to assume that yes-using biased datasets to train a neural network (NN) will cause the network to function poorly. Its important for NNs to be trained with large, diverse datasets so that they can prepare for unpredictable real-world scenarios. This point has been made in almost any piece of work pertaining to NN. That being said, I’d like to use Tensorflow to create a NN and then test the affects of bias myself. Tensorflow allows for testing and training accuracy of a model to be quantified easily, and you can use python modules to make nifty little graphs that show results.\nIts also important to mention that the software of real self-driving cars, like those from Waymo or Audi, are not opensource (no surprise there). So any model created by a novice Tensorflow user like myself is a far cry from the object-detection systems in actual self-driving cars. That being said, the point here is to test how biased training data could affect an object detection system and practice using Tensorflow along the way.\nScript Breakdown The object detection script I wrote using Tensorflow consisted of two datasets (one to train the model and one to test the model), a convolutional neural network (CNN), and a diagram to display the accuracy of the model. The sections of the script are:\nimport modules create training dataset create testing dataset standardize the data create + compile the model data augmentation model summary train the model visualize results More robust object detection scripts often use bounding boxes (little boxes that surround an item and usually name it) to recognize unique objects in an image. However, my script doesn’t implement bounding boxes and instead classifies images as either having an obstacle, or not.\nImport Modules + Datasets Since I’m using python to write my script, the first step is to import all the modules needed:\nimport os import PIL import PIL.Image import tensorflow as tf import numpy as np from tensorflow import keras import matplotlib.pyplot as plt from tensorflow.keras import layers from tensorflow.keras.models import Sequential\nWith that out of the way, I next create my datasets. There are a few ways to approach datasets for your own ML script. Here I will only be discussing datasets of images since thats what I’m familiar with, but other types of datasets do exist. I ended up using Kaggle to write my script, which allows files to be imported from your computer and added to a dataset. The path to that dataset is stored in Kaggle, and can be copied/pasted into your script to be accessed. You can also use Tensorflow’s Google Storage Bucket option to store and reference a dataset, or you can use one of Tensorflow’s own datasets, among other options.\nIn most ML scripts, you only use one dataset, and use a percentage of it to train the model and the leftover percentage to test (typically 80%train/20%test). If you train and test a model on the same dataset without it splitting into training/testing sections, accuracy metrics will say the model is 100% accurate since you aren’t testing with new data (I learned this the hard way). For my model however, I wanted one testing dataset and one training dataset that could be corrupted later on.\nTo create the testing and training datasets I used Tensorflow’s “image_dataset_from_directory” option. I first set the image’s height and width so they would all be a uniform size. I also set the batch size, which will I’ll explain later:\nbatch_size = 32 img_height = 180 img_width = 180\nWhen creating the datasets, you first specify the location of your dataset (in this case, the path that Kaggle provided me). Then, seed provides a random seed for shuffling and transformation. The validation_split represents the percentage of data you want to use to train the model and test the model. Here, a validation_split value of 0.2 means that 80% of the training dataset will be used to train the model and 20% of the testing dataset will be used to test (at this point in time the corrupt images have not been added to the testing dataset, so the two datasets are identical and the split should be used). Finally I then set the subset name, the image sizes, and the batch size.\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory( \"../input/training-dataset\", seed=123, validation_split=0.20, subset=\"training\", image_size=(img_height, img_width), batch_size=batch_size)\nval_ds = tf.keras.preprocessing.image_dataset_from_directory( \"../input/testing-dataset\", seed=123, subset=\"validation\", validation_split=0.20, image_size=(img_height, img_width), batch_size=batch_size)\nOne thing to consider when using data to train a NN is how the model will learn “what is what”. An untrained model can’t make its own conclusions, and piping random images into a model won’t teach it anything. Data should be split into classes of whatever you want to model to learn to differentiate. In my case, I had two classes: “obstacle” and “noobstacle”. In my datasets I included two folders, one for each class, and inside each folder I placed the respective images. I included a line in my script to double-check the classes were being read-in correctly:\nclass_names = train_ds.class_names print(class_names)\nStandardize, Creating and Compiling the Model Now that the datasets and classes of data are working, the model itself can be created. Like in my first Capstone post, I will be using the Sequential model, relu activation function, and Adam optimizer. However, for this script I have created a convolutional neural network (CNN).\nA CNN is a class of deep neural networks, and are commonly used to process images since they resemble the organization of an animal’s visual cortex. CNNs take in tensors of shapes, in this case the image height, width, and an RGB value of 3 (color). These aspects are then assigned importance through weights and biases in order for the model to differentiate one from another.\nIn Tensorflow, you create a NN through Conv2D and MaxPooling2D layers that each output a tensor of shape. The densest (largest) layers are placed at the top, and the layers shrink as you go down. The number of output channels for each Conv2D layer is controlled by the first argument of that layer. At the end of the model, Flatten and dense layers perform classification by “unrolling” the 3D images into 1D and using what the NN has learned to classify.\nTo create the CNN the data should first be normalized. This has to do with the mathematics of ML, but this normalization layer basically helps standardize the data.\nnormalization_layer = layers.experimental.preprocessing.Rescaling(1./255)\nNext, I defined the model layers itself:\nnum_classes = 2\nmodel = Sequential([ layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)), layers.Conv2D(16, 3, padding='same', activation='relu'), layers.MaxPooling2D(), layers.Conv2D(32, 3, padding='same', activation='relu'), layers.MaxPooling2D(), layers.Conv2D(64, 3, padding='same', activation='relu'), layers.MaxPooling2D(), layers.Dropout(0.2), layers.Flatten(), layers.Dense(128, activation='relu'), layers.Dense(num_classes) ])\nIn my CNN I added a Dropout layer. This is one of the ways I improved the performance of my model. A Dropout layer randomly sets input to 0 at each layer while training. The inputs that aren’t set to 0 are scaled up by 1. This helps prevent overfitting, which occurs when a model conforms too tightly to training data and doesn’t learn enough to make predictions on new data.\nmodel.summary() helps to visualize what the layers of the CNN look like:\nTo compile the model I used model.compile:\nmodel.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\nIn addition to a Dropout layer, I also used data augmentation to reduce overfitting. Data augmentation makes multiple different images from one training image, by flipping or zooming in on different sections of the image. This helps add in more training data without having to manually add pictures to my datasets.\ndata_augmentation = keras.Sequential( [ layers.experimental.preprocessing.RandomFlip(\"horizontal\", input_shape=(img_height, img_width, 3)), layers.experimental.preprocessing.RandomRotation(0.1), layers.experimental.preprocessing.RandomZoom(0.1), ] )\nTrain and Visualize Training and testing your model with Tensorflow can be done in a few lines of code:\nepochs=27 history = model.fit( train_ds, validation_data=val_ds, epochs=epochs )\nYou’ll notice that here epochs are set to 27. I explained back/forward propagation in my first Capstone post, but epochs are basically the number of times your NN loops through its layers to learn. Batch size, which we set earlier, defines how much of your dataset is put into your model at a time. This is a good way to save computation resources, so you aren’t trying to put a dataset of potentially millions of images into your model at a time.\nHowever, if you set the number of epochs too low, the NN won’t learn enough. If there are too many epochs, the NN can become too familiar with only the training data and overfit when tested. Unfortunately, there is no easy formula to determine how many epochs to use. I just monitored the performance of my NN with different numbers of epochs, and settled on 27.\nWhat can help you determine the number of epochs to use, along with check your model’s accuracy, is visualizing your results. In my script I used matplotlib.pyplot to make a graph.\nacc = history.history['accuracy'] val_acc = history.history['val_accuracy']\nloss = history.history['loss'] val_loss = history.history['val_loss']\nepochs_range = range(epochs)\nplt.figure(figsize=(8, 8)) plt.subplot(1, 2, 1) plt.plot(epochs_range, acc, label='Training Accuracy') plt.plot(epochs_range, val_acc, label='Testing Accuracy') plt.legend(loc='lower right') plt.title('Training and Testing Accuracy')\nplt.subplot(1, 2, 2) plt.plot(epochs_range, loss, label='Training Loss') plt.plot(epochs_range, val_loss, label='Testing Loss') plt.legend(loc='upper right') plt.title('Training and Testing Loss') plt.show()\nWrapping it Up In this case, the two most important metrics are the training accuracy and the testing accuracy. The training accuracy for my model was nearly 100%, while the testing accuracy was 72-80% (this varied between runs of my script). To put it into perspective, 50% accuracy would be guessing-since the model has only two options to choose from (obstacle or no obstacle). I can live with 72-80% accuracy, although the large difference between my training and testing accuracy means that my model is overfitting. To help fight overfitting, I most likely need to add more images to my dataset (most image datasets have 10,000+ images, mine have 200) or change the type of activation function/optimizer I am using.\nFrom the graph, you can also see that the training loss (prediction error) drastically lowers over epochs, while testing loss doesn’t significantly lower. This is another result of overfitting, although my model does work much better with the inclusion of data augmentation and dropout.\nThe next step is to edit my model slightly to be more robust, then begin corrupting testing images and experimenting. For now, I have a moderately accurate object detection CNN that would almost certainly cause a real self-driving car to crash, but it works for me.\nThe full script can be found on my Github or on Kaggle.\n",
  "wordCount" : "1868",
  "inLanguage": "en",
  "datePublished": "2020-11-11T00:00:00Z",
  "dateModified": "2020-11-11T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Olivia Snowden"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "//localhost:1313/capstone-pt2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Olivia Snowden's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "//localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="//localhost:1313/" accesskey="h" title="Olivia Snowden&#39;s Blog (Alt + H)">Olivia Snowden&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="//localhost:1313/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="//localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://github.com/adityatelange/hugo-PaperMod/wiki/" title="WiKi">
                    <span>WiKi</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="//localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="//localhost:1313/post/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      NEURAL NETWORKS &amp; TENSORFLOW - Capstone Pt.2
    </h1>
    <div class="post-meta"><span title='2020-11-11 00:00:00 +0000 UTC'>Wed, Nov 11, 2020</span>&nbsp;·&nbsp;<span>9 min</span>&nbsp;·&nbsp;<span>Olivia Snowden</span>

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#script-breakdown" aria-label="Script Breakdown">Script Breakdown</a></li>
                <li>
                    <a href="#import-modules--datasets" aria-label="Import Modules &#43; Datasets">Import Modules + Datasets</a></li>
                <li>
                    <a href="#standardize-creating-and-compiling-the-model" aria-label="Standardize, Creating and Compiling the Model">Standardize, Creating and Compiling the Model</a></li>
                <li>
                    <a href="#train-and-visualize" aria-label="Train and Visualize">Train and Visualize</a></li>
                <li>
                    <a href="#wrapping-it-up" aria-label="Wrapping it Up">Wrapping it Up</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>In my first Capstone post I gave a rundown of basic AI terms and how to use Tensorflow to create your own machine learning (ML) script.</p>
<p>I&rsquo;m using Tensorflow to write a script that can process images of roads and determine whether there is an obstacle in the road or not. Since I&rsquo;m studying self-driving cars, I&rsquo;d like to see if using biased data to train an object detection script affects the model&rsquo;s performance. To do this, I&rsquo;m using a training dataset of clear, bright road images to train a neural network and then testing that script with corrupted images of roads. If the model can&rsquo;t recognize obstacles in images that are corrupted, then the biased training dataset did have an affect on the model.</p>
<p>Its safe to assume that yes-using biased datasets to train a neural network (NN) will cause the network to function poorly.  Its important for NNs to be trained with large, diverse datasets so that they can prepare for unpredictable real-world scenarios. This point has been made in almost any piece of work pertaining to NN. That being said, I&rsquo;d like to use Tensorflow to create a NN and then test the affects of bias myself. Tensorflow allows for testing and training accuracy of a model to be quantified easily, and you can use python modules to make nifty little graphs that show results.</p>
<p>Its also important to mention that the software of real self-driving cars, like those from Waymo or Audi, are not opensource (no surprise there). So any model created by a novice Tensorflow user like myself is a far cry from the object-detection systems in actual self-driving cars. That being said, the point here is to test how biased training data could affect an object detection system and practice using Tensorflow along the way.</p>
<h4 id="script-breakdown">Script Breakdown<a hidden class="anchor" aria-hidden="true" href="#script-breakdown">#</a></h4>
<p>The object detection script I wrote using Tensorflow consisted of two datasets (one to train the model and one to test the model), a convolutional neural network (CNN), and a diagram to display the accuracy of the model. The sections of the script are:</p>
<ul>
<li>import modules</li>
<li>create training dataset</li>
<li>create testing dataset</li>
<li>standardize the data</li>
<li>create + compile the model</li>
<li>data augmentation</li>
<li>model summary</li>
<li>train the model</li>
<li>visualize results</li>
</ul>
<p>More robust object detection scripts often use bounding boxes (little boxes that surround an item and usually name it) to recognize unique objects in an image. However, my script doesn&rsquo;t implement bounding boxes and instead classifies images as either having an obstacle, or not.</p>
<h4 id="import-modules--datasets">Import Modules + Datasets<a hidden class="anchor" aria-hidden="true" href="#import-modules--datasets">#</a></h4>
<p>Since I&rsquo;m using python to write my script, the first step is to import all the modules needed:</p>
<p><code>import os   import PIL   import PIL.Image   import tensorflow as tf   import numpy as np   from tensorflow import keras   import matplotlib.pyplot as plt   from tensorflow.keras import layers   from tensorflow.keras.models import Sequential</code></p>
<p>With that out of the way, I next create my datasets. There are a few ways to approach datasets for your own ML script. Here I will only be discussing datasets of images since thats what I&rsquo;m familiar with, but other types of datasets do exist. I ended up using Kaggle to write my script, which allows files to be imported from your computer and added to a dataset. The path to that dataset is stored in Kaggle, and can be copied/pasted into your script to be accessed. You can also use Tensorflow&rsquo;s Google Storage Bucket option to store and reference a dataset, or you can use one of Tensorflow&rsquo;s own datasets, among other options.</p>
<p>In most ML scripts, you only use one dataset, and use a percentage of it to train the model and the leftover percentage to test (typically 80%train/20%test).  If you train and test a model on the same dataset without it splitting into training/testing sections, accuracy metrics will say the model is 100% accurate since you aren&rsquo;t testing with new data (I learned this the hard way). For my model however, I wanted one testing dataset and one training dataset that could be corrupted later on.</p>
<p>To create the testing and training datasets I used Tensorflow&rsquo;s &ldquo;image_dataset_from_directory&rdquo; option. I first set the image&rsquo;s height and width so they would all be a uniform size. I also set the batch size, which will I&rsquo;ll explain later:</p>
<p><code>batch_size = 32   img_height = 180   img_width = 180</code></p>
<p>When creating the datasets, you first specify the location of your dataset (in this case, the path that Kaggle provided me). Then, <code>seed</code> provides a random seed for shuffling and transformation. The <code>validation_split</code> represents the percentage of data you want to use to train the model and test the model. Here, a <code>validation_split</code> value of 0.2 means that 80% of the training dataset will be used to train the model and 20% of the testing dataset will be used to test (at this point in time the corrupt images have not been added to the testing dataset, so the two datasets are identical and the split should be used). Finally I then set the subset name, the image sizes, and the batch size.</p>
<p><code>train_ds = tf.keras.preprocessing.image_dataset_from_directory(   &quot;../input/training-dataset&quot;,   seed=123,   validation_split=0.20,   subset=&quot;training&quot;,   image_size=(img_height, img_width),   batch_size=batch_size)</code></p>
<p><code>val_ds = tf.keras.preprocessing.image_dataset_from_directory(   &quot;../input/testing-dataset&quot;,   seed=123,   subset=&quot;validation&quot;,   validation_split=0.20,   image_size=(img_height, img_width),   batch_size=batch_size)</code></p>
<p>One thing to consider when using data to train a NN is how the model will learn &ldquo;what is what&rdquo;. An untrained model can&rsquo;t make its own conclusions, and piping random images into a model won&rsquo;t teach it anything. Data should be split into classes of whatever you want to model to learn to differentiate. In my case, I had two classes: &ldquo;obstacle&rdquo; and &ldquo;noobstacle&rdquo;.  In my datasets I included two folders, one for each class, and inside each folder I placed the respective images. I included a line in my script to double-check the classes were being read-in correctly:</p>
<p><code>class_names = train_ds.class_names   print(class_names)</code></p>
<p><img alt="Classes from my datasets" loading="lazy" src="/capstone-6.png"></p>
<h4 id="standardize-creating-and-compiling-the-model">Standardize, Creating and Compiling the Model<a hidden class="anchor" aria-hidden="true" href="#standardize-creating-and-compiling-the-model">#</a></h4>
<p>Now that the datasets and classes of data are working, the model itself can be created. Like in my first Capstone post, I will be using the Sequential model, relu activation function, and Adam optimizer. However, for this script I have created a convolutional neural network (CNN).</p>
<p>A CNN is a class of deep neural networks, and are commonly used to process images since they resemble the organization of an animal&rsquo;s visual cortex. CNNs take in tensors of shapes, in this case the image height, width, and an RGB value of 3 (color). These aspects are then assigned importance through weights and biases in order for the model to differentiate one from another.</p>
<p>In Tensorflow, you create a NN through Conv2D and MaxPooling2D layers that each output a tensor of shape. The densest (largest) layers are placed at the top, and the layers shrink as you go down. The number of output channels for each Conv2D layer is controlled by the first argument of that layer. At the end of the model, Flatten and dense layers perform classification by &ldquo;unrolling&rdquo; the 3D images into 1D and using what the NN has learned to classify.</p>
<p>To create the CNN the data should first be normalized. This has to do with the mathematics of ML, but this normalization layer basically helps standardize the data.</p>
<p><code>normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)</code></p>
<p>Next, I defined the model layers itself:</p>
<p><code>num_classes = 2</code></p>
<p><code>model = Sequential([   layers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),   layers.Conv2D(16, 3, padding='same', activation='relu'),   layers.MaxPooling2D(),   layers.Conv2D(32, 3, padding='same', activation='relu'),   layers.MaxPooling2D(),   layers.Conv2D(64, 3, padding='same', activation='relu'),   layers.MaxPooling2D(),   layers.Dropout(0.2),   layers.Flatten(),   layers.Dense(128, activation='relu'),   layers.Dense(num_classes)   ])</code></p>
<p>In my CNN I added a Dropout layer. This is one of the ways I improved the performance of my model. A Dropout layer randomly sets input to 0 at each layer while training. The inputs that aren&rsquo;t set to 0 are scaled up by 1. This helps prevent overfitting, which occurs when a model conforms too tightly to training data and doesn&rsquo;t learn enough to make predictions on new data.</p>
<p>model.summary() helps to visualize what the layers of the CNN look like:</p>
<p><img alt="Output from model.summary" loading="lazy" src="/capstone-7.png"></p>
<p>To compile the model I used model.compile:</p>
<p><code>model.compile(optimizer='adam',   loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),   metrics=['accuracy'])</code></p>
<p>In addition to a Dropout layer, I also used data augmentation to reduce overfitting. Data augmentation makes multiple different images from one training image, by flipping or zooming in on different sections of the image. This helps add in more training data without having to manually add pictures to my datasets.</p>
<p><code>data_augmentation = keras.Sequential(   [   layers.experimental.preprocessing.RandomFlip(&quot;horizontal&quot;,   input_shape=(img_height,   img_width,   3)),   layers.experimental.preprocessing.RandomRotation(0.1),   layers.experimental.preprocessing.RandomZoom(0.1),   ]   )</code></p>
<p><img alt="Examples of data augmentation on one training image" loading="lazy" src="/capstone-8.png"></p>
<h4 id="train-and-visualize">Train and Visualize<a hidden class="anchor" aria-hidden="true" href="#train-and-visualize">#</a></h4>
<p>Training and testing your model with Tensorflow can be done in a few lines of code:</p>
<p><code>epochs=27   history = model.fit(   train_ds,   validation_data=val_ds,   epochs=epochs   )</code></p>
<p>You&rsquo;ll notice that here epochs are set to 27. I explained back/forward propagation in my first Capstone post, but epochs are basically the number of times your NN loops through its layers to learn. Batch size, which we set earlier, defines how much of your dataset is put into your model at a time. This is a good way to save computation resources, so you aren&rsquo;t trying to put a dataset of potentially millions of images into your model at a time.</p>
<p>However, if you set the number of epochs too low, the NN won&rsquo;t learn enough. If there are too many epochs, the NN can become too familiar with only the training data and overfit when tested. Unfortunately, there is no easy formula to determine how many epochs to use. I just monitored the performance of my NN with different numbers of epochs, and settled on 27.</p>
<p><img alt="One epoch from my model" loading="lazy" src="/capstone-9.png"></p>
<p>What can help you determine the number of epochs to use, along with check your model&rsquo;s accuracy, is visualizing your results.  In my script I used matplotlib.pyplot to make a graph.</p>
<p><code>acc = history.history['accuracy']   val_acc = history.history['val_accuracy']</code></p>
<p><code>loss = history.history['loss']   val_loss = history.history['val_loss']</code></p>
<p><code>epochs_range = range(epochs)</code></p>
<p><code>plt.figure(figsize=(8, 8))   plt.subplot(1, 2, 1)   plt.plot(epochs_range, acc, label='Training Accuracy')   plt.plot(epochs_range, val_acc, label='Testing Accuracy')   plt.legend(loc='lower right')   plt.title('Training and Testing Accuracy')</code></p>
<p><code>plt.subplot(1, 2, 2)   plt.plot(epochs_range, loss, label='Training Loss')   plt.plot(epochs_range, val_loss, label='Testing Loss')   plt.legend(loc='upper right')   plt.title('Training and Testing Loss')   plt.show()</code></p>
<p><img alt="Graph from my model" loading="lazy" src="/capstone-10.png"></p>
<h4 id="wrapping-it-up">Wrapping it Up<a hidden class="anchor" aria-hidden="true" href="#wrapping-it-up">#</a></h4>
<p>In this case, the two most important metrics are the training accuracy and the testing accuracy. The training accuracy for my model was nearly 100%, while the testing accuracy was 72-80% (this varied between runs of my script). To put it into perspective, 50% accuracy would be guessing-since the model has only two options to choose from (obstacle or no obstacle).  I can live with 72-80% accuracy, although the large difference between my training and testing accuracy means that my model is overfitting. To help fight overfitting, I most likely need to add more images to my dataset (most image datasets have 10,000+ images, mine have 200) or change the type of activation function/optimizer I am using.</p>
<p>From the graph, you can also see that the training loss (prediction error) drastically lowers over epochs, while testing loss doesn&rsquo;t significantly lower. This is another result of overfitting, although my model does work much better with the inclusion of data augmentation and dropout.</p>
<p>The next step is to edit my model slightly to be more robust, then begin corrupting testing images and experimenting. For now, I have a moderately accurate object detection CNN that would almost certainly cause a real self-driving car to crash, but it works for me.</p>
<p>The full script can be found on <a href="https://github.com/olivialauryn/capstone-object-detection">my Github</a> or on<a href="https://www.kaggle.com/oliviasnowden/obstacle-detection"> Kaggle</a>.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="//localhost:1313/tags/code/">Code</a></li>
      <li><a href="//localhost:1313/tags/ai/">AI</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="//localhost:1313/capstone-pt1/">
    <span class="title">« Prev</span>
    <br>
    <span>NEURAL NETWORKS &amp; TENSORFLOW - Capstone Pt.1</span>
  </a>
  <a class="next" href="//localhost:1313/bash-basics/">
    <span class="title">Next »</span>
    <br>
    <span>BASH BASICS</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share NEURAL NETWORKS & TENSORFLOW - Capstone Pt.2 on x"
            href="https://x.com/intent/tweet/?text=NEURAL%20NETWORKS%20%26%20TENSORFLOW%20-%20Capstone%20Pt.2&amp;url=%2f%2flocalhost%3a1313%2fcapstone-pt2%2f&amp;hashtags=code%2cAI">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share NEURAL NETWORKS & TENSORFLOW - Capstone Pt.2 on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2f%2flocalhost%3a1313%2fcapstone-pt2%2f&amp;title=NEURAL%20NETWORKS%20%26%20TENSORFLOW%20-%20Capstone%20Pt.2&amp;summary=NEURAL%20NETWORKS%20%26%20TENSORFLOW%20-%20Capstone%20Pt.2&amp;source=%2f%2flocalhost%3a1313%2fcapstone-pt2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share NEURAL NETWORKS & TENSORFLOW - Capstone Pt.2 on reddit"
            href="https://reddit.com/submit?url=%2f%2flocalhost%3a1313%2fcapstone-pt2%2f&title=NEURAL%20NETWORKS%20%26%20TENSORFLOW%20-%20Capstone%20Pt.2">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share NEURAL NETWORKS & TENSORFLOW - Capstone Pt.2 on facebook"
            href="https://facebook.com/sharer/sharer.php?u=%2f%2flocalhost%3a1313%2fcapstone-pt2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share NEURAL NETWORKS & TENSORFLOW - Capstone Pt.2 on whatsapp"
            href="https://api.whatsapp.com/send?text=NEURAL%20NETWORKS%20%26%20TENSORFLOW%20-%20Capstone%20Pt.2%20-%20%2f%2flocalhost%3a1313%2fcapstone-pt2%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share NEURAL NETWORKS & TENSORFLOW - Capstone Pt.2 on telegram"
            href="https://telegram.me/share/url?text=NEURAL%20NETWORKS%20%26%20TENSORFLOW%20-%20Capstone%20Pt.2&amp;url=%2f%2flocalhost%3a1313%2fcapstone-pt2%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share NEURAL NETWORKS & TENSORFLOW - Capstone Pt.2 on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=NEURAL%20NETWORKS%20%26%20TENSORFLOW%20-%20Capstone%20Pt.2&u=%2f%2flocalhost%3a1313%2fcapstone-pt2%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>© <a href="https://github.com/adityatelange/hugo-PaperMod/graphs/contributors">PaperMod Contributors</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
